{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time Name entity recognition on the IPU\n",
    "\n",
    "Integration of the Graphcore Intelligence Processing Unit (IPU) and the Hugging Face transformer library means that it only takes a few lines of code to perform complex tasks which require deep learning.\n",
    "\n",
    "In this notebook we perform **name entity extraction (NER)**  also known as token classification: we use natural language processing models to classify the words inside the prompt. \n",
    "\n",
    "\n",
    "The ease of use of the `pipeline` interface lets us quickly experiment with the pre-trained models and identify which one will work best.\n",
    "This simple interface means that it is extremely easy to access the fast inference performance of the IPU on your application.\n",
    "\n",
    "<img src=\"images/token_classification.png\" alt=\"Widget inference on a token classification task\" style=\"width:500px;\">\n",
    "\n",
    "While this notebook is focused on using the model (inference), our [token_classification](token_classification.ipynb) notebook will show you how to fine tune a model for a specific task using the [`datasets`](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81f9be",
   "metadata": {},
   "source": [
    "In order to run this demo you will need to have a Poplar SDK environment enabled with the PopTorch installed\n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system),  and Optimum Graphcore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's make sure your environment has the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"optimum-graphcore>=0.4, <0.5\"\n",
    "# %pip install emoji==0.6.0 gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with transformers pipelines on the IPU\n",
    "\n",
    "Lets load our model config for the IPU and get started with using pipelines to run NER on the IPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import pipelines\n",
    "inference_config = dict(layers_per_ipu=[40], ipus_per_replica=1, enable_half_partials=True,\n",
    "                        executable_cache_dir=executable_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use a model on the IPU is to use the `pipeline` function. It provides a set of models which have been validated to work on a given task. To get started choose the task and call the `pipeline` (To do: explain what the pipeline function does) function:\n",
    "\n",
    "This loads up the most basic \"ner\" model, which in this case the default is `BERT...` , learn more spesific details about the pipeline here: https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/pipelines#transformers.TokenClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_pipeline = pipelines.pipeline(\"ner\", \n",
    "                                  ipu_config=inference_config, \n",
    "                                  padding='max_length', \n",
    "                                  max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a prompt which we can use with our model pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The simplest way to use a model on the IPU is to use the `pipeline` function.\n",
    "It provides a set of models which have been validated to work on a given task. To get\n",
    "started choose the task and call the `pipeline` function\"\"\"\n",
    "out = ner_pipeline(prompt)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets give our pipeline some examples to do NER on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"My name is Wolfgang and I live in Berlin, I work for Graphcore and I really like HuggingFace\",\n",
    "    \"I'm from France and I live in the UK, John is happy there.\",\n",
    "    \"Dans Budapest, la est une grande piscine ou les gens visite. \",\n",
    "    \"The hospital was full of patients with many different diseases, many had covid-19 , flu and colds.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our model pipeline to do NER on our examples, for instance lets look at our model outputs for our first prompt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ner = ner_pipeline(examples[0])\n",
    "output_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output is messy and doesn't allow us to really quickly understand our models outputs. \n",
    "We're lucky though because we can use the gradio app to build a fast and simple app to quickly view the models outpts!\n",
    "\n",
    "Lets demo this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def app_for_pipeline(pipeline, examples=None, description=\"\", label_description=\"\"):\n",
    "    return gr.Interface(\n",
    "        fn=lambda x: dict(text=x, entities=pipeline(x)),\n",
    "        inputs = [\n",
    "            gr.Textbox(\n",
    "                label=\"Initial text\",\n",
    "                lines=3,\n",
    "                value=prompt,\n",
    "            ),\n",
    "        ],\n",
    "        outputs=gr.HighlightedText(\n",
    "            label=label_description,\n",
    "            combine_adjacent=True,\n",
    "            postprocess=True,\n",
    "            value=dict(text=prompt, entities=out)\n",
    "        ),\n",
    "        examples=examples,\n",
    "        description=description,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see how our app allows us to quickly view, test and evaluate our model and examples! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_for_pipeline(ner_pipeline, examples=[examples]).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have seen how fast and easy it is to run NER on the IPU and to build an app to make it look pretty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual model\n",
    "\n",
    "We can even quickly load and run a model which is able to do the same task but for different languages. Lets put that to action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "ner_pipeline_multilingual = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_3(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can port this model to our gradio app as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_for_pipeline(ner_pipeline_multilingual, examples=[examples]).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use our model to identify and locate food in text..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food information extraction\n",
    "\n",
    "The advantage of using pipelines on the IPU is that we can quickly load different models for different tasks! For instance lets load up this checkpoint which identifies food in text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"chambliss/distilbert-for-food-extraction\"\n",
    "ner_pipeline_3 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_3(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our new model to ID prompts related to food in some new text examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_for_pipeline(\n",
    "    ner_pipeline_3, \n",
    "    examples=[\"I went to the restaurant last night, the food was excellent. I hate roast carrots with a side of chips, what a meal!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\").launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomedical Model\n",
    "A task which may be more applicable and usable in industry could be identifying key words in medical data. Hypotehtically, if you had to queary a large database of medcial data to learn about a spesific disease this may take alot of time which could be simplified by using a NER model to help us pick out and highlight very spesific information\n",
    "\n",
    "Hence lets load up this biological medical model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"alvaroalon2/biobert_diseases_ner\"\n",
    "ner_pipeline_4 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_4(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see how this looks in gradio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_from_pipeline(\n",
    "    ner_pipeline_4, examples=[\"I'm ill, I've got a cold or the flu. It's annoying I have shivers and a fever!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows us how how fast, easy and interactive the IPU can be. \n",
    "We have been able to quickly swap out models for different purposes super fast! Also, we have been able to build an app that we can use as an interactive interface between us and the IPU to visualise our results quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################## Alex's old code ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "\n",
    "ner_pipeline_2 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "ner_pipeline_2 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_for_pipeline(ner_pipeline_2, examples=examples).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"chambliss/distilbert-for-food-extraction\"\n",
    "ner_pipeline_3 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_3(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_for_pipeline(\n",
    "    ner_pipeline_3, \n",
    "    examples=[\"I went to the restaurant last night, the food was excellent. I hate roast carrots with a side of chips, what a meal!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"alvaroalon2/biobert_diseases_ner\"\n",
    "ner_pipeline_4 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_4(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pipeline2(\n",
    "    ner_pipeline_4, examples=[\"I'm ill, I've got a cold or the flu. It's annoying I have shivers and a fever!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pipeline(pipeline, examples=None, description=\"\", label_description=\"\"):\n",
    "    return gr.Interface(\n",
    "        fn=lambda x: dict(text=x, entities=pipeline(x)),\n",
    "        inputs = [\n",
    "            gr.Textbox(\n",
    "                label=\"Initial text\",\n",
    "                lines=3,\n",
    "                value=prompt,\n",
    "            ),\n",
    "        ],\n",
    "        outputs=gr.HighlightedText(\n",
    "            label=label_description,\n",
    "            combine_adjacent=True,\n",
    "            postprocess=True,\n",
    "            value=dict(text=prompt, entities=out)\n",
    "        ),\n",
    "        examples=examples,\n",
    "        description=description,\n",
    "    )\n",
    "\n",
    "def from_pipeline2(pipeline, examples=[], description=\"\", label_description=\"\"):\n",
    "    demo = gr.Blocks(   \n",
    "        # examples=examples,\n",
    "        description=description,\n",
    "    )\n",
    "    with demo:\n",
    "        inputs = gr.Textbox(\n",
    "            label=\"Initial text\",\n",
    "            lines=3,\n",
    "            value=prompt,\n",
    "        )\n",
    "        outputs=gr.HighlightedText(\n",
    "            label=label_description,\n",
    "            combine_adjacent=True,\n",
    "            postprocess=True,\n",
    "            value=dict(text=prompt, entities=out)\n",
    "        )\n",
    "        examples_block = gr.Examples(examples=examples, inputs=inputs, outputs=outputs)\n",
    "        inputs.change(\n",
    "            fn=lambda x: dict(text=x, entities=pipeline(x)),\n",
    "            inputs=inputs, outputs=outputs, postprocess=True\n",
    "        )\n",
    "    return demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = from_pipeline2(\n",
    "    ner_pipeline_4, \n",
    "    examples=[\n",
    "        \"I'm ill, I've got a cold or an influenza. It's annoying I have shivers and a fever!\"        \n",
    "    ] + examples,\n",
    "    description=\"Try prompting it with something related to diseases\",\n",
    "    label_description=\"Diseases\",\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "46bde714a99d715eba7e507975e678b0968e7177d805932276a51e552e29fed0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
