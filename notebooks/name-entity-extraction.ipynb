{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name entity recognition on the IPU\n",
    "\n",
    "Integration of the Graphcore Intelligence Processing Unit (IPU) and the Hugging Face transformer library means that it only takes a few lines of code to perform complex tasks which require deep learning.\n",
    "\n",
    "In this notebook we perform **name entity extraction (NER)**  also known as token classification: we use natural language processing models to classify the words inside the prompt. \n",
    "\n",
    "\n",
    "The ease of use of the `pipeline` interface lets us quickly experiment with the pre-trained models and identify which one will work best.\n",
    "This simple interface means that it is extremely easy to access the fast inference performance of the IPU on your application.\n",
    "\n",
    "<img src=\"images/token_classification.png\" alt=\"Widget inference on a token classification task\" style=\"width:500px;\">\n",
    "\n",
    "While this notebook is focused on using the model (inference), our [token_classification](token_classification.ipynb) notebook will show you how to fine tune a model for a specific task using the [`datasets`](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81f9be",
   "metadata": {},
   "source": [
    "In order to run this demo you will need to have a Poplar SDK environment enabled with the PopTorch installed\n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system),  and Optimum Graphcore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's make sure your environment has the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"optimum-graphcore>=0.4, <0.5\"\n",
    "%pip install emoji==0.6.0 gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value for cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"./exe_cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with transformers pipelines on the IPU\n",
    "\n",
    "The simplest way to use a model on the IPU is to use the `pipeline` function. It provides a set of models which have been validated to work on a given task. To get started choose the task and call the `pipeline` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import pipelines\n",
    "inference_config = dict(layers_per_ipu=[40], ipus_per_replica=1, enable_half_partials=True,\n",
    "                        executable_cache_dir=executable_cache_dir)\n",
    "ner_pipeline = pipelines.pipeline(\"ner\", ipu_config=inference_config, padding='max_length', max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The simplest way to use a model on the IPU is to use the `pipeline` function.\n",
    "It provides a set of models which have been validated to work on a given task. To get\n",
    "started choose the task and call the `pipeline` function\"\"\"\n",
    "out = ner_pipeline(prompt)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    \"My name is Wolfgang and I live in Berlin, I work for Graphcore and I really like HuggingFace\",\n",
    "    prompt,\n",
    "    \"I'm from France and I live in the UK, John is happy there.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "gr.Interface.from_pipeline(ner_pipeline, examples=examples).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"vblagoje/bert-english-uncased-finetuned-pos\"\n",
    "ner_pipeline_2 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_2(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface.from_pipeline(ner_pipeline_2, examples=examples).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"chambliss/distilbert-for-food-extraction\"\n",
    "ner_pipeline_3 = pipelines.pipeline(\n",
    "    \"ner\", model=model, ipu_config=inference_config,\n",
    "    padding='max_length', max_length=256\n",
    ")\n",
    "out = ner_pipeline_3(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.Interface.from_pipeline(\n",
    "    ner_pipeline_3, examples=[\"I went to the restaurant last night, the food was excellent. I hate roast carrots with a side of chips, what a meal!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\"\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_pipeline(pipeline, examples=None, description=\"\"):\n",
    "    return gr.Interface(\n",
    "        fn=lambda x: dict(text=x, entities=pipeline(x)),\n",
    "        inputs = [\n",
    "            gr.Textbox(\n",
    "            label=\"Initial text\",\n",
    "            lines=3,\n",
    "            value=\"The quick brown fox jumped over the lazy dogs.\",\n",
    "            examples=examples,\n",
    "            description=description\n",
    "            ),\n",
    "        ],\n",
    "        outputs=gr.HighlightedText(\n",
    "            label=\"Diff\",\n",
    "            combine_adjacent=True,\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_pipeline(ner_pipeline_3, examples=[\"I went to the restaurant last night, the food was excellent. I hate roast carrots with a side of chips, what a meal!\"] + examples,\n",
    "    description=\"Try prompting me with some food-related text!\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "46bde714a99d715eba7e507975e678b0968e7177d805932276a51e552e29fed0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
