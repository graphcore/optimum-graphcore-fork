{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b804343",
   "metadata": {},
   "source": [
    "# Whisper-tiny on IPU\n",
    "\n",
    "This notebook demonstrates inference with Whisper-tiny on IPU using FP16.    \n",
    "The present version of the IPU Whisper implementation runs the encoder and the decoder on IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8698b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130d401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig, WhisperTokenizer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class IPUWhisperConf:\n",
    "    \"\"\"A data class to collect IPU-related config parameters\"\"\"\n",
    "    model_spec: str\n",
    "    layers_per_ipu: List\n",
    "    pod_type: str\n",
    "\n",
    "ipu_whisper = {\n",
    "    \"tiny\": IPUWhisperConf(model_spec='openai/whisper-tiny.en', layers_per_ipu=[8], pod_type=\"pod4\"),\n",
    "    # Larger sizes will become available in due course\n",
    "}\n",
    "model_size = \"tiny\"\n",
    "iwc = ipu_whisper[model_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89f42e",
   "metadata": {},
   "source": [
    "Max output sequence length \n",
    "- default is 448, but I couldn't fit more than 1 batch\n",
    "- 384 tokens in 30 seconds should still be ok for English, which is ~170 workds / minute for a fast speaker, and lets us fit batch size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6731f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 448\n",
    "max_length = 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1085e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", iwc.pod_type)\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/whisper_exe_cache/\") + \"whisper_inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a689797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"PVTI_OPTIONS\"]=r'{\"enable\":\"true\", \"directory\":\"/localdata/paolot/profiles/minimal\"}'\n",
    "# os.environ[\"POPLAR_ENGINE_OPTIONS\"] = f'{{\"autoReport.all\":\"true\", \"debug.allowOutOfMemory\": \"true\", \"autoReport.directory\":\"profiles\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420bdd00",
   "metadata": {},
   "source": [
    "Source file: \n",
    "\n",
    "wget https://upload.wikimedia.org/wikipedia/commons/3/3d/Barack_Obama_inauguration_speech_2009.ogg\n",
    "ffmpeg -y -i Barack_Obama_inauguration_speech_2009.ogg -ar 16000 -ac 1 BarackObama.wav\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1073fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate processor and model\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(iwc.model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(iwc.model_spec)\n",
    "ipu_config = IPUConfig(\n",
    "    executable_cache_dir=executable_cache_dir,\n",
    "    layers_per_ipu=iwc.layers_per_ipu, \n",
    "    matmul_proportion=0.1)\n",
    "pipelined_model = to_pipelined(model, ipu_config)\n",
    "pipelined_model = pipelined_model.parallelize().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e486ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "wav_array, sampling_rate = sf.read('BarackObama.wav')\n",
    "wav_array = np.array(wav_array, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_length_s = len(wav_array) / sampling_rate\n",
    "num_samples_30_sec = int(30.0 * sampling_rate)\n",
    "num_samples_to_pad = -len(wav_array) % num_samples_30_sec\n",
    "padded_wav_array = np.pad(wav_array, (0, num_samples_to_pad))\n",
    "reshaped_wav_array = padded_wav_array.reshape((len(padded_wav_array)//num_samples_30_sec, num_samples_30_sec) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{reshaped_wav_array.shape[0]} transcriptions of {reshaped_wav_array.shape[1]/sampling_rate} seconds, total duration {len(padded_wav_array)/sampling_rate} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0e1e0",
   "metadata": {},
   "source": [
    "## Batch size\n",
    "\n",
    "Still experimental. The batching is done manually via ugly for() loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1544486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e6d51",
   "metadata": {},
   "source": [
    "# Separate pre-/post- processing\n",
    "\n",
    "### Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6fff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_input_features = []\n",
    "for k in range(0, reshaped_wav_array.shape[0], batch_size):\n",
    "    wav_data = [reshaped_wav_array[k+i,:] for i in range(batch_size)]\n",
    "    all_input_features.append( processor(wav_data, return_tensors='pt',sampling_rate=16000).input_features.half() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9135d",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_sample_outputs = []\n",
    "for input_features in all_input_features:\n",
    "    sample_output = pipelined_model.generate(input_features, max_length=max_length, min_length=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420aa31",
   "metadata": {},
   "source": [
    "### Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecbbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_transcriptions = []\n",
    "for sample_output in all_sample_outputs:\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=False)\n",
    "    all_transcriptions.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_transcriptions[0]), len(all_transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1912510d",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "all_transcriptions = []\n",
    "for k in range(0, reshaped_wav_array.shape[0], batch_size):\n",
    "    wav_data = [reshaped_wav_array[k+i,:] for i in range(batch_size)]\n",
    "    input_features = processor(wav_data, return_tensors='pt',sampling_rate=16000).input_features.half()\n",
    "    sample_output = pipelined_model.generate(input_features, max_length=max_length, min_length=3)\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=False)\n",
    "    all_transcriptions.append(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_transcriptions[0]), len(all_transcriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bfa47",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Trying HF inference pipelines, in particular becuase it implements a 30-sec overlapping window (https://huggingface.co/openai/whisper-medium.en#long-form-transcription).\n",
    "\n",
    "Currently doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore import pipeline\n",
    "# NOTE: you'll need ffpmeg installed on the system (apt install ffmpeg)\n",
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07313ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.sampling_rate=16000\n",
    "feature_extractor._processor_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833fe59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\", language='english')\n",
    "pp = pipeline(\"automatic-speech-recognition\", \n",
    "              model=pipelined_model, \n",
    "              config=ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8061c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pp('Barack_Obama_inauguration_speech_2009.ogg', \n",
    "   chunk_length_s=30, \n",
    "   stride_length_s=[6,0], \n",
    "   batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f01d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelinedWhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fbf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.graphcore.models import whisper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.PipelinedWhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dab6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
