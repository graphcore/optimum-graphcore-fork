{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4cRE8IbIrIV"
   },
   "source": [
    "First of all, make sure your environment has installed the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MOsHUjgdIrIW",
    "outputId": "f84a093e-147f-470e-aad9-80fb51193c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[graphcore] in /usr/local/lib/python3.8/dist-packages (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (1.23.4)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.20.1 in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (4.24.0)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (1.10.0+cpu)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (15.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (0.10.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (21.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (1.11.1)\n",
      "Requirement already satisfied: optimum-graphcore in /usr/local/lib/python3.8/dist-packages (from optimum[graphcore]) (0.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[graphcore]) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[graphcore]) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[graphcore]) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[graphcore]) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.8.0->optimum[graphcore]) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->optimum[graphcore]) (3.0.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.20.1->optimum[graphcore]) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.20.1->optimum[graphcore]) (0.13.2)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.20.1->optimum[graphcore]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in /usr/local/lib/python3.8/dist-packages (from transformers[sentencepiece]>=4.20.1->optimum[graphcore]) (3.20.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.8/dist-packages (from coloredlogs->optimum[graphcore]) (10.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from optimum-graphcore->optimum[graphcore]) (1.9.3)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from optimum-graphcore->optimum[graphcore]) (9.3.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (from optimum-graphcore->optimum[graphcore]) (2.6.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->optimum[graphcore]) (1.2.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (1.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (0.18.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (2022.11.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (0.3.5.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (10.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets->optimum-graphcore->optimum[graphcore]) (0.70.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[graphcore]) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[graphcore]) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[graphcore]) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.8.0->optimum[graphcore]) (2019.11.28)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore->optimum[graphcore]) (22.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore->optimum[graphcore]) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore->optimum[graphcore]) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-graphcore->optimum[graphcore]) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%pip install optimum[graphcore]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up [here](https://huggingface.co/join) if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f169bf4df4594189ac7cd90cd67477f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 0%\r",
      "\r",
      "Reading package lists... 14%\r",
      "\r",
      "Reading package lists... Done\r",
      "\r\n",
      "\r",
      "Building dependency tree... 0%\r",
      "\r",
      "Building dependency tree... 0%\r",
      "\r",
      "Building dependency tree... 50%\r",
      "\r",
      "Building dependency tree... 50%\r",
      "\r",
      "Building dependency tree       \r",
      "\r\n",
      "\r",
      "Reading state information... 0%\r",
      "\r",
      "Reading state information... 0%\r",
      "\r",
      "Reading state information... Done\r",
      "\r\n",
      "git-lfs is already the newest version (2.9.2-1).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\r\n"
     ]
    }
   ],
   "source": [
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the versions of Transformers and Optimum Graphcore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n",
      "0.3.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import optimum.graphcore\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(optimum.graphcore.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod16\")\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a multiple choice task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTCFado4IrIc"
   },
   "source": [
    "In this notebook, we will see how to fine-tune one of the [ðŸ¤— Transformers](https://github.com/huggingface/transformers) model to a multiple choice task, which is the task of selecting the most plausible inputs in a given selection. The dataset used here is [SWAG](https://www.aclweb.org/anthology/D18-1009/) but you can adapt the pre-processing to any other multiple choice dataset you like, or your own data. SWAG is a dataset about commonsense reasoning, where each example describes a situation then proposes four options that could go after it. \n",
    "\n",
    "This notebook is built to run  with any model checkpoint from the [Model Hub](https://huggingface.co/models) as long as that model has a version with a mutiple choice head and is supported by Optimum Graphcore. The IPU config files of the supported models are available in Graphcore's [Hugging Face account](https://huggingface.co/Graphcore). You can also create your own IPU config file locally. \n",
    "\n",
    "In this notebook, we are using both data parallelism and pipeline parallelism (see this [tutorial](https://github.com/graphcore/tutorials/tree/master/tutorials/pytorch/tut2_efficient_data_loading) for more). Therefore the global batch size, which is the actual number of samples used for the weight update, is determined with three factors:\n",
    "- `global_batch_size = micro_batch_size * gradient_accumulation_steps * replication_factor`\n",
    "\n",
    "and replication factor is determined by `pod_type`, which will be used as a key to select the replication factor from a dictionary defined in the IPU config file. For example, the dictionary in the IPU config file [Graphcore/gpt2-small-ipu](https://huggingface.co/Graphcore/gpt2-small-ipu/blob/main/ipu_config.json) looks like this:\n",
    "- `\"replication_factor\": {\"pod4\": 1, \"pod8\": 2, \"pod16\": 4, \"pod32\": 8, \"pod64\": 16, \"default\": 1}`\n",
    "\n",
    "Depending on you model and the pod machine you are using, you might need to adjust these three batch-size-related arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"roberta-base\"\n",
    "\n",
    "ipu_config_name = \"Graphcore/roberta-base-ipu\"\n",
    "micro_batch_size = 2\n",
    "gradient_accumulation_steps = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data. This can be easily done with the functions `load_dataset`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKx2zKs5IrIq"
   },
   "source": [
    "`load_dataset` will cache the dataset to avoid downloading it again the next time you run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e92948b19a4ca78b682bb3b05b6c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5098ffd4ad5f4411906a3ef67ae6b74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d11a0b976744ff588f624944f420ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset swag/regular to /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa12d595933c41c7ab7bc21308247707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49361167d5b494fa90c1342141e9904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1c1a0e56044822aaec0b51c1258c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16da1f23f72f40db9d8b76f1ca370b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "677c00f60d2349168eac2e80496d1f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/73546 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/20006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/20005 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset swag downloaded and prepared to /root/.cache/huggingface/datasets/swag/regular/0.0.0/9640de08cdba6a1469ed3834fcab4b8ad8e38caf5d1ba5e7436d8b1fd067ad4c. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea17c853bf0480c97a8022c3250cbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 73546\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20006\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['video-id', 'fold-ind', 'startphrase', 'sent1', 'sent2', 'gold-source', 'ending0', 'ending1', 'ending2', 'ending3', 'label'],\n",
       "        num_rows: 20005\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3EtYfeHIrIz"
   },
   "source": [
    "To access an actual element, you need to select a split first, then give an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video-id': 'anetv_jkn6uvmqwh4',\n",
       " 'fold-ind': '3416',\n",
       " 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',\n",
       " 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',\n",
       " 'sent2': 'A drum line',\n",
       " 'gold-source': 'gold',\n",
       " 'ending0': 'passes by walking down the street playing their instruments.',\n",
       " 'ending1': 'has heard approaching them.',\n",
       " 'ending2': \"arrives and they're outside dancing and asleep.\",\n",
       " 'ending3': 'turns the lead singer watches the performance.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHUmphG3IrI3"
   },
   "source": [
    "To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video-id</th>\n",
       "      <th>fold-ind</th>\n",
       "      <th>startphrase</th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>gold-source</th>\n",
       "      <th>ending0</th>\n",
       "      <th>ending1</th>\n",
       "      <th>ending2</th>\n",
       "      <th>ending3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lsmdc1051_Harry_Potter_and_the_goblet_of_fire-7075</td>\n",
       "      <td>1829</td>\n",
       "      <td>He looks down and sees his feet turn into flippers. He</td>\n",
       "      <td>He looks down and sees his feet turn into flippers.</td>\n",
       "      <td>He</td>\n",
       "      <td>gold</td>\n",
       "      <td>grips the reactor into her right hand.</td>\n",
       "      <td>turns back to the zookeeper who was still pointing.</td>\n",
       "      <td>held up his hands as they become webbed.</td>\n",
       "      <td>kicks and pounds the doors.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lsmdc0008_Fargo-50029</td>\n",
       "      <td>6250</td>\n",
       "      <td>We are looking over the shoulders of two uniformed policemen who stand on either side of the door, their hands resting lightly on their holstered sidearms. One of them</td>\n",
       "      <td>We are looking over the shoulders of two uniformed policemen who stand on either side of the door, their hands resting lightly on their holstered sidearms.</td>\n",
       "      <td>One of them</td>\n",
       "      <td>gold</td>\n",
       "      <td>takes a pair of clippers and drops the bag into his left palm.</td>\n",
       "      <td>raps at the door.</td>\n",
       "      <td>is preparing in real combat.</td>\n",
       "      <td>has a couple standing on the line.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lsmdc0025_THE_LORD_OF_THE_RINGS_THE_RETURN_OF_THE_KING-61280</td>\n",
       "      <td>535</td>\n",
       "      <td>Someone moves forward, a fell light in his normally friendly eyes. Someone</td>\n",
       "      <td>Someone moves forward, a fell light in his normally friendly eyes.</td>\n",
       "      <td>Someone</td>\n",
       "      <td>gen</td>\n",
       "      <td>pick up the receiver in her mouth again.</td>\n",
       "      <td>hands him a plank.</td>\n",
       "      <td>leaps up into his passenger seat.</td>\n",
       "      <td>hands the shooter a drinks.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lsmdc3019_COLOMBIANA-9199</td>\n",
       "      <td>11698</td>\n",
       "      <td>Spinning, she backhands him with one of the brushes. He</td>\n",
       "      <td>Spinning, she backhands him with one of the brushes.</td>\n",
       "      <td>He</td>\n",
       "      <td>gold</td>\n",
       "      <td>presses a button and closes the door behind her.</td>\n",
       "      <td>drops into someone's bed and runs out of the chamber of a brownstone building.</td>\n",
       "      <td>opens a gift and pokes them with her nervous fingers.</td>\n",
       "      <td>charges, grabbing her around the waist.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lsmdc1019_Confessions_Of_A_Shopaholic-80800</td>\n",
       "      <td>9146</td>\n",
       "      <td>At home, someone runs to the fridge and frantically searches through the freezer. She</td>\n",
       "      <td>At home, someone runs to the fridge and frantically searches through the freezer.</td>\n",
       "      <td>She</td>\n",
       "      <td>gold</td>\n",
       "      <td>fits the remaining bloody balls into the laundry.</td>\n",
       "      <td>brings out a huge block of ice in the middle of which is a credit card.</td>\n",
       "      <td>limps over the crowded hallway towards pastries.</td>\n",
       "      <td>takes the suitcase out of the closet.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lsmdc3090_YOUNG_ADULT-43871</td>\n",
       "      <td>10867</td>\n",
       "      <td>Inside a convenience store, she opens a freezer case. Dolce</td>\n",
       "      <td>Inside a convenience store, she opens a freezer case.</td>\n",
       "      <td>Dolce</td>\n",
       "      <td>gen</td>\n",
       "      <td>are both wanting to be pretty.</td>\n",
       "      <td>to a corner, someone kicks off the shoe.</td>\n",
       "      <td>, someone gives them to her boss then dumps some alcohol into dough.</td>\n",
       "      <td>the slender someone, someone turns on the light.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lsmdc3017_CHRONICLE-7382</td>\n",
       "      <td>9946</td>\n",
       "      <td>As our view tilts downward and glides over the asphalt, we glimpse two of the unconscious thugs. Someone</td>\n",
       "      <td>As our view tilts downward and glides over the asphalt, we glimpse two of the unconscious thugs.</td>\n",
       "      <td>Someone</td>\n",
       "      <td>gold</td>\n",
       "      <td>drops someone's hand to her chest.</td>\n",
       "      <td>telekinetically drags one side - to - side, leaving bloody smears on the pavement.</td>\n",
       "      <td>sidles to a table next to him.</td>\n",
       "      <td>eyes the glamorous on the bed.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lsmdc3085_TRUE_GRIT-40869</td>\n",
       "      <td>16709</td>\n",
       "      <td>Silhouetted against the pale sky, someone gallops on across the dusty plain. Someone</td>\n",
       "      <td>Silhouetted against the pale sky, someone gallops on across the dusty plain.</td>\n",
       "      <td>Someone</td>\n",
       "      <td>gold</td>\n",
       "      <td>glances up at someone who hauls himself back down.</td>\n",
       "      <td>soars up the chimney and lands halfway near the closed gate.</td>\n",
       "      <td>glances at the setting sun, her face cast in its golden glow.</td>\n",
       "      <td>treads toward the lighthouse, which lights up on a second tower.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lsmdc3006_A_GOOD_DAY_TO_DIE_HARD-2947</td>\n",
       "      <td>3163</td>\n",
       "      <td>Someone checks the gun's chamber and clip, then holsters it in his waistband. Someone</td>\n",
       "      <td>Someone checks the gun's chamber and clip, then holsters it in his waistband.</td>\n",
       "      <td>Someone</td>\n",
       "      <td>gold</td>\n",
       "      <td>40, someone slips off a radio.</td>\n",
       "      <td>steps the agent, tosses his gun up, and points the gun into the direction of the other.</td>\n",
       "      <td>gradually combines to strip the mechanism tasteful.</td>\n",
       "      <td>shifts his exasperated gaze as his son helps someone to his feet and urges him onward.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>anetv_kfwwya1qzXM</td>\n",
       "      <td>5772</td>\n",
       "      <td>He describes the chords of a song and the proper finger placement to form the chords with a fingering chord diagram on the screen. The guitarist</td>\n",
       "      <td>He describes the chords of a song and the proper finger placement to form the chords with a fingering chord diagram on the screen.</td>\n",
       "      <td>The guitarist</td>\n",
       "      <td>gold</td>\n",
       "      <td>hugs the candles and continues his song.</td>\n",
       "      <td>mounts his own high drum, moves down the instrument and turns to face him.</td>\n",
       "      <td>stops playing and turns up the song.</td>\n",
       "      <td>continues to play portions of a song stopping to describe the chord placement.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example in the dataset has a context composed of a first sentence (in the field `sent1`) and an introduction to the second sentence (in the field `sent2`). Then four possible endings are given (in the fields `ending0`, `ending1`, `ending2` and `ending3`) and the model must pick the right one (indicated in the field `label`). The following function lets us visualize a give example a bit better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_one(example):\n",
    "    print(f\"Context: {example['sent1']}\")\n",
    "    print(f\"  A - {example['sent2']} {example['ending0']}\")\n",
    "    print(f\"  B - {example['sent2']} {example['ending1']}\")\n",
    "    print(f\"  C - {example['sent2']} {example['ending2']}\")\n",
    "    print(f\"  D - {example['sent2']} {example['ending3']}\")\n",
    "    print(f\"\\nGround truth: option {['A', 'B', 'C', 'D'][example['label']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Members of the procession walk down the street holding small horn brass instruments.\n",
      "  A - A drum line passes by walking down the street playing their instruments.\n",
      "  B - A drum line has heard approaching them.\n",
      "  C - A drum line arrives and they're outside dancing and asleep.\n",
      "  D - A drum line turns the lead singer watches the performance.\n",
      "\n",
      "Ground truth: option A\n"
     ]
    }
   ],
   "source": [
    "show_one(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Now it's someone's turn to rain blades on his opponent.\n",
      "  A - Someone pats his shoulder and spins wildly.\n",
      "  B - Someone lunges forward through the window.\n",
      "  C - Someone falls to the ground.\n",
      "  D - Someone rolls up his fast run from the water and tosses in the sky.\n",
      "\n",
      "Ground truth: option C\n"
     ]
    }
   ],
   "source": [
    "show_one(datasets[\"train\"][15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56827dc93e9141a9922bb8615dfad3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6138abb4e3d40208ef22c593bd95a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e19b414a534ff9b632fa90b0f310db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dce143bde442af9dd1ae641f1e6ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rowT4iCLIrJK"
   },
   "source": [
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 31414, 6, 42, 65, 3645, 328, 2, 2, 2409, 42, 3645, 1411, 19, 24, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo_0B1M2IrJM"
   },
   "source": [
    "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested.\n",
    "\n",
    "To preprocess our dataset, we will thus need the names of the columns containing the sentence(s). The following dictionary keeps track of the correspondence task to column names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can them write the function that will preprocess our samples. The tricky part is to put all the possible pairs of sentences in two big lists before passing them to the tokenizer, then un-flatten the result so that each example has four input ids, attentions masks, etc.\n",
    "\n",
    "We also define a maximum sequence length (`max_seq_length`) to either pad or truncate our samples to so that each sample is the same size. Then, when calling the `tokenizer`, we use the arguments `truncation=True`, `padding=\"max_length\"` and `max_length=max_seq_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "max_seq_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Repeat each first sentence four times to go with the four possibilities of second sentences.\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    # Grab all second sentences possible for each context.\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [[f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)]\n",
    "    \n",
    "    # Flatten everything\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_examples = tokenizer(\n",
    "        first_sentences, \n",
    "        second_sentences, \n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # Un-flatten\n",
    "    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lm8ozrJIrJR"
   },
   "source": [
    "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists of lists for each key: a list of all examples (here 5), then a list of all choices (4) and a list of input IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 4 [128, 128, 128, 128]\n"
     ]
    }
   ],
   "source": [
    "examples = datasets[\"train\"][:5]\n",
    "features = preprocess_function(examples)\n",
    "print(len(features[\"input_ids\"]), len(features[\"input_ids\"][0]), [len(x) for x in features[\"input_ids\"][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check we didn't do anything group when grouping all possibilites then unflattening, let's have a look at the decoded inputs for a given example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>A drum line passes by walking down the street playing their instruments.</s></s>Members of the procession are playing ping pong and celebrating one left each in quick.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<s>A drum line passes by walking down the street playing their instruments.</s></s>Members of the procession wait slowly towards the cadets.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<s>A drum line passes by walking down the street playing their instruments.</s></s>Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<s>A drum line passes by walking down the street playing their instruments.</s></s>Members of the procession play and go back and forth hitting the drums while the audience claps for them.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 3\n",
    "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare it to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: A drum line passes by walking down the street playing their instruments.\n",
      "  A - Members of the procession are playing ping pong and celebrating one left each in quick.\n",
      "  B - Members of the procession wait slowly towards the cadets.\n",
      "  C - Members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions.\n",
      "  D - Members of the procession play and go back and forth hitting the drums while the audience claps for them.\n",
      "\n",
      "Ground truth: option D\n"
     ]
    }
   ],
   "source": [
    "show_one(datasets[\"train\"][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "This seems alright, so we can apply this function on all the examples in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a61a5375fb4079ada4b7090d2665fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a8b9d39f304ae4a12455f02d510b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5978ebdf23fc4d9d8bcdff65119a2699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voWiw8C7IrJV"
   },
   "source": [
    "Even better, the results are automatically cached by the ðŸ¤— Datasets library to avoid spending time on this step the next time you run your notebook. The ðŸ¤— Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. ðŸ¤— Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n",
    "\n",
    "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our task is about mutliple choice, we use the `AutoModelForMultipleChoice` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. We also define the `IPUConfig`, which is a class that specifies attributes and configuration parameters to compile and put the model on the device. We initialize it with one config name or path, which we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80a4daf726b4af5a90beeafd1c183c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForMultipleChoice: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0540b5d2c6a84f24a0dd9fd0d0032e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer, IPUTrainingArguments\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)\n",
    "ipu_config = IPUConfig.from_pretrained(ipu_config_name, executable_cache_dir=executable_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CczA5lJlIrJX"
   },
   "source": [
    "The warning is telling us we are throwing away some weights (the `vocab_transform` and `vocab_layer_norm` layers) and randomly initializing some other (the `pre_classifier` and `classifier` layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "To instantiate a `IPUTrainer`, we will need to define a few more things. The most important is the [`IPUTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to customize the training. It requires one folder name, which will be used to save the checkpoints of the model, and all other arguments are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = IPUTrainingArguments(\n",
    "    f\"{model_name}-finetuned-swag\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=micro_batch_size,\n",
    "    per_device_eval_batch_size=micro_batch_size,\n",
    "    pod_type=pod_type,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    dataloader_drop_last=True,\n",
    "    push_to_hub=False,\n",
    "    # hub_model_id = f\"username-or-organization/{model_name}-finetuned-squad\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "km3pGVdTIrJc"
   },
   "source": [
    "Here we set the evaluation to be done at the end of each epoch, tweak the learning rate and use the gloal batch size defined at the top of the notebook through `micro_batch_size`, `gradient_accumulation_steps` and `pod_type`. We also customize the number of epochs for training, as well as the weight decay.\n",
    "\n",
    "The last argument to setup everything so we can push the model to the [Hub](https://huggingface.co/models) regularly during training. Remove it if you didn't follow the installation steps at the top of the notebook. If you want to save your model locally in a name that is different than the name of the repository it will be pushed, or if you want to push your model under an organization and not your name space, use the `hub_model_id` argument to set the repo name (it needs to be the full name, including your namespace: for instance `\"sgugger/bert-finetuned-swag\"` or `\"huggingface/bert-finetuned-swag\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sZOdRlRIrJd"
   },
   "source": [
    "The last thing to define for our `IPUTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, the only preprocessing we have to do is to take the argmax of our predicted logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "UmvbnJ9JIrJd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "Then we just need to pass all of this along with our datasets to the `IPUTrainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding IPU config: gradient_accumulation_steps=16\n",
      "-------------------- Device Allocation --------------------\n",
      "Embedding  --> IPU 0\n",
      "Encoder 0  --> IPU 1\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 2\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 3\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Classifier Output --> IPU 3\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "trainer = IPUTrainer(\n",
    "    model,\n",
    "    ipu_config,\n",
    "    args,\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdzABDVcIrJg"
   },
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `PipelinedRobertaForMultipleChoice.forward` and have been ignored: sent2, gold-source, fold-ind, ending1, video-id, ending3, ending2, sent1, ending0, startphrase.\n",
      "Compiling Model...\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:11<00:00]\n",
      "Compiled/Loaded model in 347.5222702920437 secs\n",
      "***** Running training *****\n",
      "  Num examples = 73546\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 2\n",
      "  Device Iterations = 1\n",
      "  Replication Factor = 4\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Total optimization steps = 1722\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f95c2f00b6a448c3ae7da3de05b0a5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1722 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3892, 'learning_rate': 4.9709639953542396e-05, 'epoch': 0.02}\n",
      "{'loss': 1.1033, 'learning_rate': 4.9419279907084783e-05, 'epoch': 0.03}\n",
      "{'loss': 1.1104, 'learning_rate': 4.9128919860627184e-05, 'epoch': 0.05}\n",
      "{'loss': 0.9539, 'learning_rate': 4.883855981416957e-05, 'epoch': 0.07}\n",
      "{'loss': 1.0036, 'learning_rate': 4.8548199767711965e-05, 'epoch': 0.09}\n",
      "{'loss': 1.003, 'learning_rate': 4.825783972125435e-05, 'epoch': 0.1}\n",
      "{'loss': 0.8866, 'learning_rate': 4.796747967479675e-05, 'epoch': 0.12}\n",
      "{'loss': 0.9367, 'learning_rate': 4.767711962833915e-05, 'epoch': 0.14}\n",
      "{'loss': 0.8064, 'learning_rate': 4.7386759581881534e-05, 'epoch': 0.16}\n",
      "{'loss': 0.8734, 'learning_rate': 4.709639953542393e-05, 'epoch': 0.17}\n",
      "{'loss': 0.7134, 'learning_rate': 4.680603948896632e-05, 'epoch': 0.19}\n",
      "{'loss': 0.9928, 'learning_rate': 4.6515679442508716e-05, 'epoch': 0.21}\n",
      "{'loss': 0.8884, 'learning_rate': 4.62253193960511e-05, 'epoch': 0.23}\n",
      "{'loss': 0.8715, 'learning_rate': 4.59349593495935e-05, 'epoch': 0.24}\n",
      "{'loss': 0.6634, 'learning_rate': 4.564459930313589e-05, 'epoch': 0.26}\n",
      "{'loss': 0.8501, 'learning_rate': 4.5354239256678285e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8242, 'learning_rate': 4.506387921022068e-05, 'epoch': 0.3}\n",
      "{'loss': 0.8506, 'learning_rate': 4.4773519163763066e-05, 'epoch': 0.31}\n",
      "{'loss': 0.9305, 'learning_rate': 4.448315911730546e-05, 'epoch': 0.33}\n",
      "{'loss': 0.8011, 'learning_rate': 4.4192799070847854e-05, 'epoch': 0.35}\n",
      "{'loss': 1.0321, 'learning_rate': 4.390243902439025e-05, 'epoch': 0.37}\n",
      "{'loss': 0.719, 'learning_rate': 4.361207897793264e-05, 'epoch': 0.38}\n",
      "{'loss': 1.0877, 'learning_rate': 4.332171893147503e-05, 'epoch': 0.4}\n",
      "{'loss': 0.7377, 'learning_rate': 4.303135888501742e-05, 'epoch': 0.42}\n",
      "{'loss': 0.6036, 'learning_rate': 4.2740998838559817e-05, 'epoch': 0.44}\n",
      "{'loss': 0.8799, 'learning_rate': 4.245063879210221e-05, 'epoch': 0.45}\n",
      "{'loss': 0.8468, 'learning_rate': 4.21602787456446e-05, 'epoch': 0.47}\n",
      "{'loss': 0.8825, 'learning_rate': 4.186991869918699e-05, 'epoch': 0.49}\n",
      "{'loss': 0.9173, 'learning_rate': 4.157955865272939e-05, 'epoch': 0.51}\n",
      "{'loss': 0.8602, 'learning_rate': 4.128919860627178e-05, 'epoch': 0.52}\n",
      "{'loss': 0.705, 'learning_rate': 4.099883855981417e-05, 'epoch': 0.54}\n",
      "{'loss': 0.7171, 'learning_rate': 4.070847851335656e-05, 'epoch': 0.56}\n",
      "{'loss': 0.907, 'learning_rate': 4.0418118466898954e-05, 'epoch': 0.57}\n",
      "{'loss': 0.7552, 'learning_rate': 4.012775842044135e-05, 'epoch': 0.59}\n",
      "{'loss': 0.7996, 'learning_rate': 3.983739837398374e-05, 'epoch': 0.61}\n",
      "{'loss': 0.7108, 'learning_rate': 3.9547038327526136e-05, 'epoch': 0.63}\n",
      "{'loss': 0.7799, 'learning_rate': 3.925667828106852e-05, 'epoch': 0.64}\n",
      "{'loss': 0.8907, 'learning_rate': 3.8966318234610924e-05, 'epoch': 0.66}\n",
      "{'loss': 0.7501, 'learning_rate': 3.867595818815331e-05, 'epoch': 0.68}\n",
      "{'loss': 0.6893, 'learning_rate': 3.8385598141695705e-05, 'epoch': 0.7}\n",
      "{'loss': 0.6993, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.71}\n",
      "{'loss': 0.8938, 'learning_rate': 3.780487804878049e-05, 'epoch': 0.73}\n",
      "{'loss': 0.6727, 'learning_rate': 3.751451800232289e-05, 'epoch': 0.75}\n",
      "{'loss': 0.7556, 'learning_rate': 3.7224157955865274e-05, 'epoch': 0.77}\n",
      "{'loss': 0.779, 'learning_rate': 3.693379790940767e-05, 'epoch': 0.78}\n",
      "{'loss': 0.8234, 'learning_rate': 3.664343786295006e-05, 'epoch': 0.8}\n",
      "{'loss': 0.778, 'learning_rate': 3.6353077816492456e-05, 'epoch': 0.82}\n",
      "{'loss': 0.6683, 'learning_rate': 3.606271777003484e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6602, 'learning_rate': 3.577235772357724e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta-base-finetuned-swag/checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7158, 'learning_rate': 3.548199767711963e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------------------- Device Allocation --------------------\n",
      "Embedding  --> IPU 0\n",
      "Encoder 0  --> IPU 1\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 2\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 3\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Classifier Output --> IPU 3\n",
      "-----------------------------------------------------------\n",
      "Configuration saved in roberta-base-finetuned-swag/checkpoint-500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7057, 'learning_rate': 3.5191637630662025e-05, 'epoch': 0.89}\n",
      "{'loss': 0.8031, 'learning_rate': 3.490127758420442e-05, 'epoch': 0.91}\n",
      "{'loss': 1.0509, 'learning_rate': 3.4610917537746806e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6161, 'learning_rate': 3.43205574912892e-05, 'epoch': 0.94}\n",
      "{'loss': 0.6646, 'learning_rate': 3.4030197444831594e-05, 'epoch': 0.96}\n",
      "{'loss': 0.7688, 'learning_rate': 3.373983739837399e-05, 'epoch': 0.98}\n",
      "{'loss': 0.579, 'learning_rate': 3.344947735191638e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `PoptorchPipelinedRobertaForMultipleChoice.forward` and have been ignored: sent2, gold-source, fold-ind, ending1, video-id, ending3, ending2, sent1, ending0, startphrase.\n",
      "Compiling Model...\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\n",
      "Graph compilation:   3%|â–Ž         | 3/100 [00:03<01:43]\u001b[A\n",
      "Graph compilation:   4%|â–         | 4/100 [00:03<01:20]\u001b[A\n",
      "Graph compilation:   6%|â–Œ         | 6/100 [00:04<00:57]\u001b[A\n",
      "Graph compilation:   7%|â–‹         | 7/100 [00:08<02:25]\u001b[A\n",
      "Graph compilation:  15%|â–ˆâ–Œ        | 15/100 [00:08<00:34]\u001b[A\n",
      "Graph compilation:  18%|â–ˆâ–Š        | 18/100 [00:09<00:31]\u001b[A\n",
      "Graph compilation:  20%|â–ˆâ–ˆ        | 20/100 [00:10<00:31]\u001b[A\n",
      "Graph compilation:  22%|â–ˆâ–ˆâ–       | 22/100 [00:10<00:25]\u001b[A\n",
      "Graph compilation:  24%|â–ˆâ–ˆâ–       | 24/100 [00:13<00:40]\u001b[A\n",
      "Graph compilation:  25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:15<01:07]\u001b[A\n",
      "Graph compilation:  26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:16<01:06]\u001b[A\n",
      "Graph compilation:  27%|â–ˆâ–ˆâ–‹       | 27/100 [00:18<01:10]\u001b[A\n",
      "Graph compilation:  28%|â–ˆâ–ˆâ–Š       | 28/100 [00:19<01:07]\u001b[A\n",
      "Graph compilation:  29%|â–ˆâ–ˆâ–‰       | 29/100 [00:19<01:04]\u001b[A\n",
      "Graph compilation:  30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:20<01:06]\u001b[A\n",
      "Graph compilation:  31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:22<01:10]\u001b[A\n",
      "Graph compilation:  32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:22<01:07]\u001b[A\n",
      "Graph compilation:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:23<00:56]\u001b[A\n",
      "Graph compilation:  34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:25<01:22]\u001b[A\n",
      "Graph compilation:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:26<01:04]\u001b[A\n",
      "Graph compilation:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:26<00:47]\u001b[A\n",
      "Graph compilation:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:26<00:36]\u001b[A\n",
      "Graph compilation:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:26<00:30]\u001b[A\n",
      "Graph compilation:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:29<00:41]\u001b[A\n",
      "Graph compilation:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:29<00:39]\u001b[A\n",
      "Graph compilation:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:30<00:21]\u001b[A\n",
      "Graph compilation:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:31<00:16]\u001b[A\n",
      "Graph compilation:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:31<00:18]\u001b[A\n",
      "Graph compilation:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:33<00:23]\u001b[A\n",
      "Graph compilation:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:35<00:28]\u001b[A\n",
      "Graph compilation:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:40<00:45]\u001b[A\n",
      "Graph compilation:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:40<00:27]\u001b[A\n",
      "Graph compilation:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:41<00:26]\u001b[A\n",
      "Graph compilation:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:41<00:20]\u001b[A\n",
      "Graph compilation:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:43<00:25]\u001b[A\n",
      "Graph compilation:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:44<00:07]\u001b[A\n",
      "Graph compilation:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:45<00:08]\u001b[A\n",
      "Graph compilation:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:45<00:06]\u001b[A\n",
      "Graph compilation:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:45<00:04]\u001b[A\n",
      "Graph compilation:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:48<00:08]\u001b[A\n",
      "Graph compilation:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:48<00:07]\u001b[A\n",
      "Graph compilation:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:50<00:08]\u001b[A\n",
      "Graph compilation:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:51<00:09]\u001b[A\n",
      "Graph compilation:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [01:03<00:30]\u001b[A\n",
      "Graph compilation:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [01:05<00:25]\u001b[A\n",
      "Graph compilation:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [01:05<00:18]\u001b[A\n",
      "Graph compilation:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [01:06<00:15]\u001b[A\n",
      "Graph compilation:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [01:09<00:08]\u001b[A\n",
      "Graph compilation:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [01:09<00:03]\u001b[A\n",
      "Graph compilation:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [01:18<00:07]\u001b[A\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:19<00:00][A\n",
      "Compiled/Loaded model in 108.70018595457077 secs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20006\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.51123046875, 'eval_accuracy': 0.8108999729156494, 'eval_runtime': 19.7182, 'eval_samples_per_second': 1014.292, 'eval_steps_per_second': 25.357, 'epoch': 1.0}\n",
      "{'loss': 0.5345, 'learning_rate': 3.315911730545877e-05, 'epoch': 1.01}\n",
      "{'loss': 0.5943, 'learning_rate': 3.286875725900116e-05, 'epoch': 1.03}\n",
      "{'loss': 0.5724, 'learning_rate': 3.2578397212543556e-05, 'epoch': 1.05}\n",
      "{'loss': 0.4031, 'learning_rate': 3.228803716608595e-05, 'epoch': 1.06}\n",
      "{'loss': 0.7032, 'learning_rate': 3.199767711962834e-05, 'epoch': 1.08}\n",
      "{'loss': 0.6286, 'learning_rate': 3.170731707317073e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6502, 'learning_rate': 3.1416957026713125e-05, 'epoch': 1.11}\n",
      "{'loss': 0.7305, 'learning_rate': 3.112659698025552e-05, 'epoch': 1.13}\n",
      "{'loss': 0.6624, 'learning_rate': 3.083623693379791e-05, 'epoch': 1.15}\n",
      "{'loss': 0.561, 'learning_rate': 3.05458768873403e-05, 'epoch': 1.17}\n",
      "{'loss': 0.4692, 'learning_rate': 3.0255516840882698e-05, 'epoch': 1.18}\n",
      "{'loss': 0.5452, 'learning_rate': 2.9965156794425088e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4487, 'learning_rate': 2.9674796747967482e-05, 'epoch': 1.22}\n",
      "{'loss': 0.557, 'learning_rate': 2.9384436701509873e-05, 'epoch': 1.24}\n",
      "{'loss': 0.5405, 'learning_rate': 2.9094076655052267e-05, 'epoch': 1.25}\n",
      "{'loss': 0.3758, 'learning_rate': 2.880371660859466e-05, 'epoch': 1.27}\n",
      "{'loss': 0.497, 'learning_rate': 2.851335656213705e-05, 'epoch': 1.29}\n",
      "{'loss': 0.5473, 'learning_rate': 2.8222996515679445e-05, 'epoch': 1.31}\n",
      "{'loss': 0.5309, 'learning_rate': 2.7932636469221835e-05, 'epoch': 1.32}\n",
      "{'loss': 0.5591, 'learning_rate': 2.764227642276423e-05, 'epoch': 1.34}\n",
      "{'loss': 0.6176, 'learning_rate': 2.735191637630662e-05, 'epoch': 1.36}\n",
      "{'loss': 0.5611, 'learning_rate': 2.7061556329849014e-05, 'epoch': 1.38}\n",
      "{'loss': 0.6845, 'learning_rate': 2.6771196283391408e-05, 'epoch': 1.39}\n",
      "{'loss': 0.5615, 'learning_rate': 2.6480836236933798e-05, 'epoch': 1.41}\n",
      "{'loss': 0.4364, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}\n",
      "{'loss': 0.7498, 'learning_rate': 2.5900116144018583e-05, 'epoch': 1.45}\n",
      "{'loss': 0.5629, 'learning_rate': 2.5609756097560977e-05, 'epoch': 1.46}\n",
      "{'loss': 0.64, 'learning_rate': 2.5319396051103367e-05, 'epoch': 1.48}\n",
      "{'loss': 0.4423, 'learning_rate': 2.502903600464576e-05, 'epoch': 1.5}\n",
      "{'loss': 0.5448, 'learning_rate': 2.4738675958188155e-05, 'epoch': 1.52}\n",
      "{'loss': 0.564, 'learning_rate': 2.4448315911730546e-05, 'epoch': 1.53}\n",
      "{'loss': 0.618, 'learning_rate': 2.415795586527294e-05, 'epoch': 1.55}\n",
      "{'loss': 0.5286, 'learning_rate': 2.3867595818815333e-05, 'epoch': 1.57}\n",
      "{'loss': 0.5524, 'learning_rate': 2.3577235772357724e-05, 'epoch': 1.59}\n",
      "{'loss': 0.5707, 'learning_rate': 2.3286875725900118e-05, 'epoch': 1.6}\n",
      "{'loss': 0.7005, 'learning_rate': 2.299651567944251e-05, 'epoch': 1.62}\n",
      "{'loss': 0.4338, 'learning_rate': 2.2706155632984902e-05, 'epoch': 1.64}\n",
      "{'loss': 0.8631, 'learning_rate': 2.2415795586527293e-05, 'epoch': 1.66}\n",
      "{'loss': 0.5707, 'learning_rate': 2.2125435540069687e-05, 'epoch': 1.67}\n",
      "{'loss': 0.5228, 'learning_rate': 2.183507549361208e-05, 'epoch': 1.69}\n",
      "{'loss': 0.6373, 'learning_rate': 2.1544715447154475e-05, 'epoch': 1.71}\n",
      "{'loss': 0.5032, 'learning_rate': 2.1254355400696865e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta-base-finetuned-swag/checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.56, 'learning_rate': 2.096399535423926e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------------------- Device Allocation --------------------\n",
      "Embedding  --> IPU 0\n",
      "Encoder 0  --> IPU 1\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 2\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 3\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Classifier Output --> IPU 3\n",
      "-----------------------------------------------------------\n",
      "Configuration saved in roberta-base-finetuned-swag/checkpoint-1000/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5629, 'learning_rate': 2.067363530778165e-05, 'epoch': 1.76}\n",
      "{'loss': 0.5001, 'learning_rate': 2.038327526132404e-05, 'epoch': 1.78}\n",
      "{'loss': 0.4997, 'learning_rate': 2.0092915214866434e-05, 'epoch': 1.79}\n",
      "{'loss': 0.578, 'learning_rate': 1.9802555168408828e-05, 'epoch': 1.81}\n",
      "{'loss': 0.5013, 'learning_rate': 1.9512195121951222e-05, 'epoch': 1.83}\n",
      "{'loss': 0.4452, 'learning_rate': 1.9221835075493612e-05, 'epoch': 1.85}\n",
      "{'loss': 0.5156, 'learning_rate': 1.8931475029036006e-05, 'epoch': 1.86}\n",
      "{'loss': 0.6229, 'learning_rate': 1.8641114982578397e-05, 'epoch': 1.88}\n",
      "{'loss': 0.4592, 'learning_rate': 1.835075493612079e-05, 'epoch': 1.9}\n",
      "{'loss': 0.504, 'learning_rate': 1.806039488966318e-05, 'epoch': 1.92}\n",
      "{'loss': 0.5656, 'learning_rate': 1.7770034843205575e-05, 'epoch': 1.93}\n",
      "{'loss': 0.4979, 'learning_rate': 1.747967479674797e-05, 'epoch': 1.95}\n",
      "{'loss': 0.5768, 'learning_rate': 1.718931475029036e-05, 'epoch': 1.97}\n",
      "{'loss': 0.5202, 'learning_rate': 1.6898954703832754e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `PoptorchPipelinedRobertaForMultipleChoice.forward` and have been ignored: sent2, gold-source, fold-ind, ending1, video-id, ending3, ending2, sent1, ending0, startphrase.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20006\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.443359375, 'eval_accuracy': 0.8269500136375427, 'eval_runtime': 20.5227, 'eval_samples_per_second': 974.528, 'eval_steps_per_second': 24.363, 'epoch': 2.0}\n",
      "{'loss': 0.4903, 'learning_rate': 1.6608594657375144e-05, 'epoch': 2.0}\n",
      "{'loss': 0.5191, 'learning_rate': 1.6318234610917538e-05, 'epoch': 2.02}\n",
      "{'loss': 0.4093, 'learning_rate': 1.602787456445993e-05, 'epoch': 2.04}\n",
      "{'loss': 0.4608, 'learning_rate': 1.5737514518002326e-05, 'epoch': 2.06}\n",
      "{'loss': 0.2969, 'learning_rate': 1.5447154471544717e-05, 'epoch': 2.07}\n",
      "{'loss': 0.535, 'learning_rate': 1.5156794425087109e-05, 'epoch': 2.09}\n",
      "{'loss': 0.3082, 'learning_rate': 1.4866434378629501e-05, 'epoch': 2.11}\n",
      "{'loss': 0.3538, 'learning_rate': 1.4576074332171893e-05, 'epoch': 2.13}\n",
      "{'loss': 0.5002, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.14}\n",
      "{'loss': 0.4397, 'learning_rate': 1.3995354239256678e-05, 'epoch': 2.16}\n",
      "{'loss': 0.3271, 'learning_rate': 1.3704994192799073e-05, 'epoch': 2.18}\n",
      "{'loss': 0.341, 'learning_rate': 1.3414634146341466e-05, 'epoch': 2.2}\n",
      "{'loss': 0.3814, 'learning_rate': 1.3124274099883858e-05, 'epoch': 2.21}\n",
      "{'loss': 0.5349, 'learning_rate': 1.283391405342625e-05, 'epoch': 2.23}\n",
      "{'loss': 0.561, 'learning_rate': 1.2543554006968642e-05, 'epoch': 2.25}\n",
      "{'loss': 0.4298, 'learning_rate': 1.2253193960511034e-05, 'epoch': 2.26}\n",
      "{'loss': 0.4092, 'learning_rate': 1.1962833914053427e-05, 'epoch': 2.28}\n",
      "{'loss': 0.3979, 'learning_rate': 1.1672473867595819e-05, 'epoch': 2.3}\n",
      "{'loss': 0.3696, 'learning_rate': 1.1382113821138211e-05, 'epoch': 2.32}\n",
      "{'loss': 0.3594, 'learning_rate': 1.1091753774680605e-05, 'epoch': 2.33}\n",
      "{'loss': 0.3709, 'learning_rate': 1.0801393728222997e-05, 'epoch': 2.35}\n",
      "{'loss': 0.4227, 'learning_rate': 1.051103368176539e-05, 'epoch': 2.37}\n",
      "{'loss': 0.2507, 'learning_rate': 1.0220673635307783e-05, 'epoch': 2.39}\n",
      "{'loss': 0.3462, 'learning_rate': 9.930313588850176e-06, 'epoch': 2.4}\n",
      "{'loss': 0.3929, 'learning_rate': 9.639953542392568e-06, 'epoch': 2.42}\n",
      "{'loss': 0.3112, 'learning_rate': 9.34959349593496e-06, 'epoch': 2.44}\n",
      "{'loss': 0.234, 'learning_rate': 9.059233449477352e-06, 'epoch': 2.46}\n",
      "{'loss': 0.4877, 'learning_rate': 8.768873403019745e-06, 'epoch': 2.47}\n",
      "{'loss': 0.2947, 'learning_rate': 8.478513356562137e-06, 'epoch': 2.49}\n",
      "{'loss': 0.3761, 'learning_rate': 8.188153310104529e-06, 'epoch': 2.51}\n",
      "{'loss': 0.3782, 'learning_rate': 7.897793263646923e-06, 'epoch': 2.53}\n",
      "{'loss': 0.3379, 'learning_rate': 7.607433217189315e-06, 'epoch': 2.54}\n",
      "{'loss': 0.389, 'learning_rate': 7.317073170731707e-06, 'epoch': 2.56}\n",
      "{'loss': 0.4099, 'learning_rate': 7.0267131242741005e-06, 'epoch': 2.58}\n",
      "{'loss': 0.4783, 'learning_rate': 6.736353077816493e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta-base-finetuned-swag/checkpoint-1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3563, 'learning_rate': 6.445993031358885e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-------------------- Device Allocation --------------------\n",
      "Embedding  --> IPU 0\n",
      "Encoder 0  --> IPU 1\n",
      "Encoder 1  --> IPU 1\n",
      "Encoder 2  --> IPU 1\n",
      "Encoder 3  --> IPU 1\n",
      "Encoder 4  --> IPU 2\n",
      "Encoder 5  --> IPU 2\n",
      "Encoder 6  --> IPU 2\n",
      "Encoder 7  --> IPU 2\n",
      "Encoder 8  --> IPU 3\n",
      "Encoder 9  --> IPU 3\n",
      "Encoder 10 --> IPU 3\n",
      "Encoder 11 --> IPU 3\n",
      "Classifier Output --> IPU 3\n",
      "-----------------------------------------------------------\n",
      "Configuration saved in roberta-base-finetuned-swag/checkpoint-1500/ipu_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3431, 'learning_rate': 6.155632984901278e-06, 'epoch': 2.63}\n",
      "{'loss': 0.2043, 'learning_rate': 5.86527293844367e-06, 'epoch': 2.65}\n",
      "{'loss': 0.3082, 'learning_rate': 5.574912891986063e-06, 'epoch': 2.67}\n",
      "{'loss': 0.3106, 'learning_rate': 5.2845528455284555e-06, 'epoch': 2.68}\n",
      "{'loss': 0.3059, 'learning_rate': 4.994192799070848e-06, 'epoch': 2.7}\n",
      "{'loss': 0.3219, 'learning_rate': 4.703832752613241e-06, 'epoch': 2.72}\n",
      "{'loss': 0.3045, 'learning_rate': 4.413472706155633e-06, 'epoch': 2.74}\n",
      "{'loss': 0.4955, 'learning_rate': 4.123112659698026e-06, 'epoch': 2.75}\n",
      "{'loss': 0.2783, 'learning_rate': 3.832752613240418e-06, 'epoch': 2.77}\n",
      "{'loss': 0.3906, 'learning_rate': 3.542392566782811e-06, 'epoch': 2.79}\n",
      "{'loss': 0.4786, 'learning_rate': 3.2520325203252037e-06, 'epoch': 2.8}\n",
      "{'loss': 0.384, 'learning_rate': 2.961672473867596e-06, 'epoch': 2.82}\n",
      "{'loss': 0.3712, 'learning_rate': 2.6713124274099885e-06, 'epoch': 2.84}\n",
      "{'loss': 0.5561, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}\n",
      "{'loss': 0.4863, 'learning_rate': 2.090592334494774e-06, 'epoch': 2.87}\n",
      "{'loss': 0.3462, 'learning_rate': 1.8002322880371663e-06, 'epoch': 2.89}\n",
      "{'loss': 0.3065, 'learning_rate': 1.5098722415795587e-06, 'epoch': 2.91}\n",
      "{'loss': 0.2868, 'learning_rate': 1.2195121951219514e-06, 'epoch': 2.93}\n",
      "{'loss': 0.3334, 'learning_rate': 9.291521486643438e-07, 'epoch': 2.94}\n",
      "{'loss': 0.5206, 'learning_rate': 6.387921022067365e-07, 'epoch': 2.96}\n",
      "{'loss': 0.4363, 'learning_rate': 3.4843205574912896e-07, 'epoch': 2.98}\n",
      "{'loss': 0.4403, 'learning_rate': 5.807200929152149e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `PoptorchPipelinedRobertaForMultipleChoice.forward` and have been ignored: sent2, gold-source, fold-ind, ending1, video-id, ending3, ending2, sent1, ending0, startphrase.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20006\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44482421875, 'eval_accuracy': 0.8381500244140625, 'eval_runtime': 20.0499, 'eval_samples_per_second': 997.513, 'eval_steps_per_second': 24.938, 'epoch': 3.0}\n",
      "{'train_runtime': 833.1948, 'train_samples_per_second': 264.543, 'train_steps_per_second': 2.067, 'train_loss': 0.5913507902386851, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1722, training_loss=0.5913507902386851, metrics={'train_runtime': 833.1948, 'train_samples_per_second': 264.543, 'train_steps_per_second': 2.067, 'train_loss': 0.5913507902386851, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check how our model did with the `evaluate` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `PoptorchPipelinedRobertaForMultipleChoice.forward` and have been ignored: sent2, gold-source, fold-ind, ending1, video-id, ending3, ending2, sent1, ending0, startphrase.\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wY82caEX3l_i"
   },
   "source": [
    "You can now upload the result of the training to the Hub, just execute this instruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now share this model with all your friends, family, favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForMultipleChoice\n",
    "\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"sgugger/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Multiple choice on SWAG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
