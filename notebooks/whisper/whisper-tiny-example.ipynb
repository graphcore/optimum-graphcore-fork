{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b804343",
   "metadata": {},
   "source": [
    "# Whisper-tiny on IPU\n",
    "\n",
    "This notebook demonstrates inference with Whisper-tiny on IPU using FP16. Presently runs on branch whisper/poc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f8f495-7729-47fb-a969-a17e4c6688f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Graphcore Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e1e009c-e3b3-4fbe-8009-5a261b779b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POPLAR version 3.2.0 (1513789a51)\n",
      "clang version 15.0.0 (bab932b4fc4cdb58bb009370384b2c41579bd9d9)\n"
     ]
    }
   ],
   "source": [
    "# Be sure you're running Poplar version 3.2\n",
    "# If not, go back to the paperspace instance launch page, select 'advanced options', then select:\n",
    "# graphcore/pytorch-jupyter:3.2.0-ubuntu-20.04-20230314 as the container name\n",
    "!popc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc9dcb3-289a-4049-94f3-6752d5149a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/graphcore/optimum-graphcore-fork.git\n",
      "  Cloning https://github.com/graphcore/optimum-graphcore-fork.git to /tmp/pip-req-build-je5k6k0t\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/graphcore/optimum-graphcore-fork.git /tmp/pip-req-build-je5k6k0t\n",
      "  Resolved https://github.com/graphcore/optimum-graphcore-fork.git to commit 769986b10fdfba4b4526057ea27c4d667edeb0b5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers<0.13 in /usr/local/lib/python3.8/dist-packages (0.12.1)\n",
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting torch@ https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl\n",
      "  Using cached https://download.pytorch.org/whl/cpu/torch-1.13.1%2Bcpu-cp38-cp38-linux_x86_64.whl (199.1 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "Collecting diffusers[torch]==0.12.1\n",
      "  Using cached diffusers-0.12.1-py3-none-any.whl (604 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting optimum==1.6.1\n",
      "  Using cached optimum-1.6.1-py3-none-any.whl (222 kB)\n",
      "Collecting pillow\n",
      "  Using cached Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Collecting importlib-metadata\n",
      "  Downloading importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting accelerate>=0.11.0\n",
      "  Using cached accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.20.1 in /usr/local/lib/python3.8/dist-packages (from optimum==1.6.1->optimum-graphcore==0.6.0.dev0) (4.20.1)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/tokenizers-0.12.1.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.7/442.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.14\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting typing-extensions>=4.1.1\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting pooch<1.7,>=1.0\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Collecting decorator>=4.3.0\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting msgpack>=1.0\n",
      "  Using cached msgpack-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "Collecting audioread>=2.1.9\n",
      "  Using cached audioread-3.0.0-py3-none-any.whl\n",
      "Collecting scipy>=1.2.0\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting soxr>=0.3.2\n",
      "  Using cached soxr-0.3.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting numba>=0.51.0\n",
      "  Using cached numba-0.56.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "Collecting scikit-learn>=0.20.0\n",
      "  Using cached scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "Collecting lazy-loader>=0.1\n",
      "  Using cached lazy_loader-0.2-py3-none-any.whl (8.6 kB)\n",
      "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3\n",
      "  Using cached numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3\n",
      "  Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.51.0->librosa) (67.6.0)\n",
      "Collecting importlib-metadata\n",
      "  Using cached importlib_metadata-6.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting requests>=2.19.0\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.9/195.9 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/usr/local/lib/python3.8/dist-packages/tokenizers-0.12.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: msgpack, appdirs, zipp, urllib3, typing-extensions, threadpoolctl, pycparser, packaging, numpy, llvmlite, lazy-loader, joblib, idna, decorator, charset-normalizer, certifi, audioread, soxr, scipy, requests, importlib-metadata, cffi, soundfile, scikit-learn, pooch, numba, librosa\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.20.1 requires filelock, which is not installed.\n",
      "transformers 4.20.1 requires huggingface-hub<1.0,>=0.1.0, which is not installed.\n",
      "transformers 4.20.1 requires pyyaml>=5.1, which is not installed.\n",
      "transformers 4.20.1 requires regex!=2019.12.17, which is not installed.\n",
      "transformers 4.20.1 requires tokenizers!=0.11.3,<0.13,>=0.11.1, which is not installed.\n",
      "transformers 4.20.1 requires tqdm>=4.27, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires datasets, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires optimum, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires pillow, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires sentencepiece, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires tokenizers, which is not installed.\n",
      "optimum-graphcore 0.5.0 requires torch, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 audioread-3.0.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 decorator-5.1.1 idna-3.4 importlib-metadata-6.1.0 joblib-1.2.0 lazy-loader-0.2 librosa-0.10.0.post2 llvmlite-0.39.1 msgpack-1.0.5 numba-0.56.4 numpy-1.23.5 packaging-23.0 pooch-1.6.0 pycparser-2.21 requests-2.28.2 scikit-learn-1.2.2 scipy-1.10.1 soundfile-0.12.1 soxr-0.3.4 threadpoolctl-3.1.0 typing-extensions-4.5.0 urllib3-1.26.15 zipp-3.15.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs. Whisper features not yet on main branch.\n",
    "%pip install git+https://github.com/graphcore/optimum-graphcore-fork.git \"tokenizers<0.13\" \"transformers==4.25.1\"\n",
    "# %pip install git+https://github.com/graphcore/optimum-graphcore-fork.git@whisper/poc \"tokenizers<0.13\"\n",
    "%pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9f018-9db2-4c33-bd3b-ace3b7ebd49d",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a4e559-988d-46ad-8bfb-4611b3a0cc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.20.1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generic imports\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# IPU-specific imports\n",
    "import poptorch\n",
    "from optimum.graphcore import IPUConfig\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "# HF imports\n",
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e5a30-0911-4527-b12d-05bd439d6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing automatic caching of executables\n",
    "del os.environ['POPTORCH_CACHE_DIR']\n",
    "del os.environ['POPLAR_EXECUTABLE_CACHE_DIR']\n",
    "# os.environ[\"DECODER_POPLAR_ENGINE_OPTIONS\"] = f'{{\"autoReport.all\":\"true\", \"debug.allowOutOfMemory\": \"true\", \"autoReport.directory\":\"/tmp/profile1\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339ffcb-dc8e-40da-8f4a-148c81421667",
   "metadata": {},
   "source": [
    "#### Global data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class IPUWhisperConf:\n",
    "    \"\"\"A data class to collect IPU-related config parameters\"\"\"\n",
    "    model_spec: str\n",
    "    ipus_per_replica: int\n",
    "    pod_type: str\n",
    "\n",
    "ipu_whisper = {\n",
    "    \"tiny\": IPUWhisperConf(model_spec='openai/whisper-tiny.en', ipus_per_replica=2, pod_type=\"pod4\"),\n",
    "    # Larger sizes will become available in due course\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global config\n",
    "iwc = ipu_whisper[\"tiny\"]\n",
    "max_length = 448\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", iwc.pod_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate processor and model\n",
    "processor = WhisperProcessor.from_pretrained(iwc.model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(iwc.model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bef594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "test_idx = 4\n",
    "\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_features = processor(ds[test_idx][\"audio\"][\"array\"], \n",
    "                           return_tensors=\"pt\",\n",
    "                           sampling_rate=ds[test_idx]['audio']['sampling_rate']).input_features.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e0e09f-def0-49a4-b06c-bfbe4be5d09d",
   "metadata": {},
   "source": [
    "### Whisper Benchmarking: adjust parameters for different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e76e5-4e01-46a5-85df-534bf7219b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 3\n",
    "batch_size = 2\n",
    "replication_factor = 1\n",
    "\n",
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dce2bca-b840-4b96-9534-51c69b99542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = IPUConfig(executable_cache_dir=None, ipus_per_replica=iwc.ipus_per_replica, matmul_proportion=0.1, inference_replication_factor=replication_factor)\n",
    "\n",
    "pipelined_model = to_pipelined(model, ipu_config).parallelize(for_generation=True).half()\n",
    "\n",
    "sample_output = pipelined_model.generate(\n",
    "    input_features.repeat(batch_size,1,1), \n",
    "    max_length=max_length, \n",
    "    min_length=3, \n",
    "    num_beams=num_beams)\n",
    "\n",
    "transcription = processor.batch_decode(sample_output, skip_special_tokens=False)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f8257-5179-4ae9-abb0-1db2eb547025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark():\n",
    "    sample_output = pipelined_model.generate(\n",
    "        input_features.repeat(batch_size,1,1), \n",
    "        max_length=max_length, \n",
    "        min_length=3, \n",
    "        num_beams=num_beams)\n",
    "    transcription = processor.batch_decode(sample_output, skip_special_tokens=False)\n",
    "\n",
    "print(f\"Running with num_beams={num_beams}, timing steps of batch size {batch_size}, running on {2*replication_factor} IPUs\")\n",
    "%timeit -n 10 benchmark()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
