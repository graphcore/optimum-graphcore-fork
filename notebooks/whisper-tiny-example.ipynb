{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b804343",
   "metadata": {},
   "source": [
    "# Whisper-tiny on IPU\n",
    "\n",
    "This notebook demonstrates inference with Whisper-tiny on IPU using FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8698b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241ca065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/paolot/popsdk-venvs/poplar_sdk-ubuntu_20_04-3.1.0+1205-58b501c780/3.1.0+1205_poptorch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2023 Graphcore Ltd. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "\"\"\" Run inference on a ðŸ¤— Whisper model \"\"\"\n",
    "\n",
    "from optimum.utils import logging\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import poptorch\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer,IPUTrainingArguments\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c672ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130d401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class IPUWhisperConf:\n",
    "    \"\"\"A data class to collect IPU-related config parameters\"\"\"\n",
    "    model_spec: str\n",
    "    layers_per_ipu: List\n",
    "    pod_type: str\n",
    "\n",
    "ipu_whisper = {\n",
    "    \"tiny\": IPUWhisperConf(model_spec='openai/whisper-tiny.en', layers_per_ipu=[8], pod_type=\"pod4\"),\n",
    "    \"small\": IPUWhisperConf(model_spec='openai/whisper-small', layers_per_ipu=[6,6,6,6], pod_type=\"pod4\"),\n",
    "    \"large\": IPUWhisperConf(model_spec='openai/whisper-large-v2', layers_per_ipu=[4,4,4,4,4,4,4,4, 4,4,4,4,4,4,4,4], pod_type=\"pod16\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6731f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"tiny\"\n",
    "iwc = ipu_whisper[model_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edac9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate processor and model\n",
    "processor = WhisperProcessor.from_pretrained(iwc.model_spec)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(iwc.model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bef594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset librispeech_asr_dummy (/home/paolot/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
     ]
    }
   ],
   "source": [
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "input_features = processor(ds[0][\"audio\"][\"array\"], \n",
    "                           return_tensors=\"pt\",\n",
    "                           sampling_rate=ds[0]['audio']['sampling_rate']).input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1085e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", iwc.pod_type)\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/whisper_exe_cache/\") + \"whisper_inference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd474186",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = IPUConfig(executable_cache_dir=executable_cache_dir, layers_per_ipu=iwc.layers_per_ipu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1073fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelined_model = to_pipelined(model, ipu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca66754",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelined_model = pipelined_model.parallelize().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17947b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|notimestamps|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.<|endoftext|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_output = pipelined_model.generate(input_features, max_length=448, min_length=3)\n",
    "transcription = processor.batch_decode(sample_output, skip_special_tokens=False)[0]\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dc6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
