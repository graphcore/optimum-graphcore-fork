{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb00796b",
   "metadata": {},
   "source": [
    "# wav2vec 2.0 Fine-Tuning on IPU\n",
    "\n",
    "This notebook will demonstrate how to fine-tune a pre-trained wav2vec 2.0 model with PyTorch on the Graphcore IPU-POD16 system. We will use a \"wav2vec2-base\" model and fine-tune for a CTC downstream task using LibriSpeech.\n",
    "\n",
    "We will show how to use a wav2vec 2.0 model written in PyTorch from the ü§ó`transformers` library from HuggingFace and paralllize it easily using the ü§ó`optimum-graphcore` library.\n",
    "\n",
    "### Background\n",
    "\n",
    "ASR (Automatic Speech Recognition), the task of transcribing audio automatically, has historically required large amounts of labelled data. Additionally, these systems had predominantly used fixed feature extraction methods which do not learn from the raw signal, e.g., STFT, or Mel-Frequency. Research conducted by Facebook AI (now Meta AI) demonstrates a ‚Äúframework for self-supervised learning for speech representations‚Äù. In other words, a pre-training phase and architecture which can learn feature representations, and their relationships, by leveraging large amounts of unlabelled, raw audio data.  \n",
    "\n",
    "There are two phases to training: pre-training on unlabelled data, and fine-tuning on a down-stream task. In the original literature the model is fine-tuned for CTC (connectionist temporal classification), which is an ASR task. The consistent modules between pre-training and fine-tuning are what you‚Äôd expect to see in a CTC system; it has feature extraction, and an encoder. But, unlike many models of the past, the feature extraction is a convolutional neural network, which makes it trainable. Following that there is a BERT-style encoder where a large convolutional block is used before the first layers, rather than using sinusoidal positional encoding.  \n",
    "\n",
    "### Environment\n",
    "\n",
    "Requirements:\n",
    "- A Poplar SDK environment enabled (see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) guide for your IPU system)\n",
    "- Python packages installed with `python -m pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae7f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1887 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2820 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [30.4 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1273 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1772 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.7 kB]\n",
      "Get:17 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [972 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2350 kB]\n",
      "Fetched 24.6 MB in 2s (16.4 MB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "29 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libflac8 libvorbisenc2\n",
      "The following NEW packages will be installed:\n",
      "  libflac8 libsndfile1 libvorbisenc2\n",
      "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
      "Need to get 344 kB of archives.\n",
      "After this operation, 1554 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libflac8 amd64 1.3.3-1ubuntu0.1 [103 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libvorbisenc2 amd64 1.3.6-2ubuntu1 [70.7 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libsndfile1 amd64 1.0.28-7ubuntu0.1 [170 kB]\n",
      "Fetched 344 kB in 0s (4811 kB/s)\n",
      "Selecting previously unselected package libflac8:amd64.\r\n",
      "(Reading database ... \r",
      "(Reading database ... 5%\r",
      "(Reading database ... 10%\r",
      "(Reading database ... 15%\r",
      "(Reading database ... 20%\r",
      "(Reading database ... 25%\r",
      "(Reading database ... 30%\r",
      "(Reading database ... 35%\r",
      "(Reading database ... 40%\r",
      "(Reading database ... 45%\r",
      "(Reading database ... 50%\r",
      "(Reading database ... 55%\r",
      "(Reading database ... 60%\r",
      "(Reading database ... 65%\r",
      "(Reading database ... 70%\r",
      "(Reading database ... 75%\r",
      "(Reading database ... 80%\r",
      "(Reading database ... 85%\r",
      "(Reading database ... 90%\r",
      "(Reading database ... 95%\r",
      "(Reading database ... 100%\r",
      "(Reading database ... 56130 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libflac8_1.3.3-1ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking libflac8:amd64 (1.3.3-1ubuntu0.1) ...\r\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\r\n",
      "Preparing to unpack .../libvorbisenc2_1.3.6-2ubuntu1_amd64.deb ...\r\n",
      "Unpacking libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\r\n",
      "Selecting previously unselected package libsndfile1:amd64.\r\n",
      "Preparing to unpack .../libsndfile1_1.0.28-7ubuntu0.1_amd64.deb ...\r\n",
      "Unpacking libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\r\n",
      "Setting up libflac8:amd64 (1.3.3-1ubuntu0.1) ...\r\n",
      "Setting up libvorbisenc2:amd64 (1.3.6-2ubuntu1) ...\r\n",
      "Setting up libsndfile1:amd64 (1.0.28-7ubuntu0.1) ...\r\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt update\n",
    "apt-get install libsndfile1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add16b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting optimum-graphcore<0.5,>0.4\n",
      "  Downloading optimum_graphcore-0.4.2-py3-none-any.whl (179 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.8/179.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==0.10.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-0.10.0%2Bcpu-cp38-cp38-linux_x86_64.whl (2.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jiwer\n",
      "  Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.8/dist-packages (from torchaudio==0.10.0+cpu->-r requirements.txt (line 3)) (1.10.0+cpu)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0->torchaudio==0.10.0+cpu->-r requirements.txt (line 3)) (4.4.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pillow\n",
      "  Downloading Pillow-9.3.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.20.1\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m451.7/451.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting optimum\n",
      "  Downloading optimum-1.5.1-py3-none-any.whl (187 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m187.3/187.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy\n",
      "  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.8.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (4.64.1)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.19.1\n",
      "  Downloading scikit_learn-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.8/dist-packages (from librosa->-r requirements.txt (line 4)) (5.1.1)\n",
      "Collecting numba>=0.45.1\n",
      "  Downloading numba-0.56.4-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=0.14\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting levenshtein==0.20.2\n",
      "  Downloading Levenshtein-0.20.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n",
      "  Downloading rapidfuzz-2.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.8/dist-packages (from soundfile->-r requirements.txt (line 6)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 6)) (2.21)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.45.1->librosa->-r requirements.txt (line 4)) (5.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.45.1->librosa->-r requirements.txt (line 4)) (65.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (3.0.9)\n",
      "Collecting appdirs>=1.3.0\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.7\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.0/36.0 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.20.1\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2.1.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (1.25.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers[sentencepiece]>=4.20.1\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.23.0-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading transformers-4.21.0-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<=3.20.1\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.45.1->librosa->-r requirements.txt (line 4)) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2022.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (2.8.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.6/532.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-graphcore<0.5,>0.4->-r requirements.txt (line 1)) (1.14.0)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23702 sha256=9accb721311d0e0af03d826d13cfecb768707213dbdfc3b8c21bfc53f38984ae\n",
      "  Stored in directory: /root/.cache/pip/wheels/74/25/48/ad94b69151b78e9aeba6850da119f04eda1c811d22fcf4b32d\n",
      "Successfully built audioread\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tokenizers, sentencepiece, mpmath, appdirs, xxhash, urllib3, threadpoolctl, sympy, regex, rapidfuzz, protobuf, pillow, numpy, multidict, llvmlite, joblib, humanfriendly, fsspec, frozenlist, filelock, dill, audioread, async-timeout, yarl, torchaudio, soundfile, scipy, pyarrow, pandas, numba, multiprocess, levenshtein, coloredlogs, aiosignal, scikit-learn, responses, resampy, pooch, jiwer, huggingface-hub, aiohttp, transformers, librosa, datasets, optimum, optimum-graphcore\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Uninstalling urllib3-1.25.8:\n",
      "      Successfully uninstalled urllib3-1.25.8\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.2 audioread-3.0.0 coloredlogs-15.0.1 datasets-2.7.1 dill-0.3.6 filelock-3.8.1 frozenlist-1.3.3 fsspec-2022.11.0 huggingface-hub-0.11.1 humanfriendly-10.0 jiwer-2.5.1 joblib-1.2.0 levenshtein-0.20.2 librosa-0.9.2 llvmlite-0.39.1 mpmath-1.2.1 multidict-6.0.3 multiprocess-0.70.14 numba-0.56.4 numpy-1.23.5 optimum-1.5.1 optimum-graphcore-0.4.2 pandas-1.5.2 pillow-9.3.0 pooch-1.6.0 protobuf-3.20.1 pyarrow-10.0.1 rapidfuzz-2.13.3 regex-2022.10.31 resampy-0.4.2 responses-0.18.0 scikit-learn-1.1.3 scipy-1.9.3 sentencepiece-0.1.97 soundfile-0.11.0 sympy-1.11.1 threadpoolctl-3.1.0 tokenizers-0.12.1 torchaudio-0.10.0+cpu transformers-4.20.1 urllib3-1.26.13 xxhash-3.1.0 yarl-1.8.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf7002b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version\r\n",
      "-------------------- -----------\r\n",
      "agate                1.6.3\r\n",
      "agate-dbf            0.2.2\r\n",
      "agate-excel          0.2.5\r\n",
      "agate-sql            0.5.8\r\n",
      "aiohttp              3.8.3\r\n",
      "aiosignal            1.3.1\r\n",
      "anyio                3.6.2\r\n",
      "appdirs              1.4.4\r\n",
      "argon2-cffi          21.3.0\r\n",
      "argon2-cffi-bindings 21.2.0\r\n",
      "async-timeout        4.0.2\r\n",
      "attrs                22.1.0\r\n",
      "audioread            3.0.0\r\n",
      "awscli               1.26.0\r\n",
      "Babel                2.10.3\r\n",
      "backcall             0.2.0\r\n",
      "beautifulsoup4       4.11.1\r\n",
      "bleach               5.0.1\r\n",
      "botocore             1.28.0\r\n",
      "certifi              2019.11.28\r\n",
      "cffi                 1.15.1\r\n",
      "chardet              3.0.4\r\n",
      "charset-normalizer   2.1.1\r\n",
      "colorama             0.4.4\r\n",
      "coloredlogs          15.0.1\r\n",
      "csvkit               1.0.7\r\n",
      "datasets             2.7.1\r\n",
      "dbfread              2.0.7\r\n",
      "dbus-python          1.2.16\r\n",
      "decorator            5.1.1\r\n",
      "defusedxml           0.7.1\r\n",
      "dill                 0.3.6\r\n",
      "distro               1.4.0\r\n",
      "docutils             0.16\r\n",
      "entrypoints          0.4\r\n",
      "et-xmlfile           1.1.0\r\n",
      "fastjsonschema       2.16.2\r\n",
      "filelock             3.8.1\r\n",
      "frozenlist           1.3.3\r\n",
      "fsspec               2022.11.0\r\n",
      "future               0.18.2\r\n",
      "greenlet             1.1.3.post0\r\n",
      "huggingface-hub      0.11.1\r\n",
      "humanfriendly        10.0\r\n",
      "idna                 2.8\r\n",
      "importlib-metadata   5.0.0\r\n",
      "importlib-resources  5.10.0\r\n",
      "ipykernel            5.5.6\r\n",
      "ipython              7.16.3\r\n",
      "ipython-genutils     0.2.0\r\n",
      "ipywidgets           8.0.2\r\n",
      "isodate              0.6.1\r\n",
      "jedi                 0.17.2\r\n",
      "Jinja2               3.1.2\r\n",
      "jiwer                2.5.1\r\n",
      "jmespath             1.0.1\r\n",
      "joblib               1.2.0\r\n",
      "json5                0.9.10\r\n",
      "jsonschema           4.16.0\r\n",
      "jupyter              1.0.0\r\n",
      "jupyter_client       7.4.4\r\n",
      "jupyter-console      6.4.4\r\n",
      "jupyter_core         4.11.2\r\n",
      "jupyter-server       1.21.0\r\n",
      "jupyterlab           3.2.7\r\n",
      "jupyterlab-pygments  0.2.2\r\n",
      "jupyterlab_server    2.16.1\r\n",
      "jupyterlab-widgets   3.0.3\r\n",
      "leather              0.3.4\r\n",
      "Levenshtein          0.20.2\r\n",
      "librosa              0.9.2\r\n",
      "llvmlite             0.39.1\r\n",
      "MarkupSafe           2.1.1\r\n",
      "mistune              2.0.4\r\n",
      "mpmath               1.2.1\r\n",
      "multidict            6.0.3\r\n",
      "multiprocess         0.70.14\r\n",
      "nbclassic            0.4.5\r\n",
      "nbclient             0.7.0\r\n",
      "nbconvert            7.2.2\r\n",
      "nbformat             5.7.0\r\n",
      "nest-asyncio         1.5.6\r\n",
      "notebook             6.5.1\r\n",
      "notebook_shim        0.2.0\r\n",
      "numba                0.56.4\r\n",
      "numpy                1.23.5\r\n",
      "olefile              0.46\r\n",
      "openpyxl             3.0.10\r\n",
      "optimum              1.5.1\r\n",
      "optimum-graphcore    0.4.2\r\n",
      "packaging            21.3\r\n",
      "pandas               1.5.2\r\n",
      "pandocfilters        1.5.0\r\n",
      "parsedatetime        2.4\r\n",
      "parso                0.7.1\r\n",
      "pexpect              4.8.0\r\n",
      "pickleshare          0.7.5\r\n",
      "Pillow               9.3.0\r\n",
      "pip                  22.3\r\n",
      "pkgutil_resolve_name 1.3.10\r\n",
      "pooch                1.6.0\r\n",
      "poptorch             3.0.0+86945\r\n",
      "prometheus-client    0.15.0\r\n",
      "prompt-toolkit       3.0.31\r\n",
      "protobuf             3.20.1\r\n",
      "ptyprocess           0.7.0\r\n",
      "pyarrow              10.0.1\r\n",
      "pyasn1               0.4.8\r\n",
      "pycparser            2.21\r\n",
      "Pygments             2.13.0\r\n",
      "PyGObject            3.36.0\r\n",
      "pyparsing            3.0.9\r\n",
      "pyrsistent           0.18.1\r\n",
      "python-dateutil      2.8.2\r\n",
      "python-slugify       6.1.2\r\n",
      "pytimeparse          1.1.8\r\n",
      "pytz                 2022.5\r\n",
      "PyYAML               5.4.1\r\n",
      "pyzmq                24.0.1\r\n",
      "qtconsole            5.3.2\r\n",
      "QtPy                 2.2.1\r\n",
      "rapidfuzz            2.13.3\r\n",
      "regex                2022.10.31\r\n",
      "requests             2.28.1\r\n",
      "resampy              0.4.2\r\n",
      "responses            0.18.0\r\n",
      "rsa                  4.7.2\r\n",
      "s3transfer           0.6.0\r\n",
      "scikit-learn         1.1.3\r\n",
      "scipy                1.9.3\r\n",
      "Send2Trash           1.8.0\r\n",
      "sentencepiece        0.1.97\r\n",
      "setuptools           65.5.0\r\n",
      "six                  1.14.0\r\n",
      "sniffio              1.3.0\r\n",
      "soundfile            0.11.0\r\n",
      "soupsieve            2.3.2.post1\r\n",
      "SQLAlchemy           1.4.42\r\n",
      "ssh-import-id        5.10\r\n",
      "sympy                1.11.1\r\n",
      "terminado            0.17.0\r\n",
      "text-unidecode       1.3\r\n",
      "threadpoolctl        3.1.0\r\n",
      "tinycss2             1.2.1\r\n",
      "tokenizers           0.12.1\r\n",
      "torch                1.10.0+cpu\r\n",
      "torchaudio           0.10.0+cpu\r\n",
      "tornado              6.2\r\n",
      "tqdm                 4.64.1\r\n",
      "traitlets            5.5.0\r\n",
      "transformers         4.20.1\r\n",
      "typing_extensions    4.4.0\r\n",
      "urllib3              1.26.13\r\n",
      "wcwidth              0.2.5\r\n",
      "webencodings         0.5.1\r\n",
      "websocket-client     1.4.1\r\n",
      "wheel                0.34.2\r\n",
      "widgetsnbextension   4.0.3\r\n",
      "xlrd                 2.0.1\r\n",
      "xxhash               3.1.0\r\n",
      "yarl                 1.8.2\r\n",
      "zipp                 3.10.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8733e4e",
   "metadata": {},
   "source": [
    "To run this Jupyter notebook on a remote IPU machine:\n",
    "1. Enable a Poplar SDK environment \n",
    "(see the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html) \n",
    " guide for your IPU system) and install required packages with `python -m pip install -r requirements.txt`\n",
    "2. In the same environment, install the Jupyter notebook server: `python -m pip install notebook`\n",
    "3. Launch a Jupyter Server on a specific port: `jupyter-notebook --no-browser --port <port number>`\n",
    "4. Connect via SSH to your remote machine, forwarding your chosen port:\n",
    "`ssh -NL <port number>:localhost:<port number> <your username>@<remote machine>`\n",
    "\n",
    "For more details about this process, or if you need troubleshooting, \n",
    "see our [guide on using IPUs from Jupyter notebooks](../../standard_tools/using_jupyter/README.md).\"\n",
    "\n",
    "### Graphcore Hugging Face models\n",
    "Hugging Face provides convenient access to pre-trained transformer models. The partnership between Hugging Face and Graphcore allows us to run these models on the IPU.\n",
    "\n",
    "Hugging Face models ported to the IPU can be found on the Graphcore organisation page on Hugging Face. \n",
    "\n",
    "### Utility imports\n",
    "We start by importing the utilities that will be used later in the tutorial: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dae5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import DatasetDict, load_dataset, load_metric\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "from optimum.graphcore import IPUConfig, IPUTrainer\n",
    "from optimum.graphcore import IPUTrainingArguments\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForCTC,\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Wav2Vec2Processor,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a47cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8313f3a",
   "metadata": {},
   "source": [
    "Values for machine size and cache directories can be configured through environment variables or directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod16\")\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\")\n",
    "checkpoint_directory = Path(os.getenv(\"CHECKPOINT_DIR\", \"/tmp\")) / \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3baf3ee",
   "metadata": {},
   "source": [
    "## Preparing the LibriSpeech Dataset\n",
    "\n",
    "The ü§ó`datasets` library from HuggingFace can be used to conventiuently load the LibriSpeech dataset, as well as provide easy to use tool to process the data.\n",
    "\n",
    "First we are going to create a `DatasetDict` dictionary to handle our data, and then load the LibriSpeech splits for training and validation. For this notebook we will use `train.100` which is 100 hours of clean training data. Section C of the appendix in the [paper](https://arxiv.org/abs/2006.11477) suggests that fine-tuning a `Base` model can yield 6.1% WER without an additional language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72874ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"train.100\")\n",
    "raw_datasets[\"eval\"] = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abca91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ac40f",
   "metadata": {},
   "source": [
    "### Text Normalisation\n",
    "\n",
    "Using the package `map` function, any special characters are removed from the transcription. The resultant transcript is then lower-cased. These two processes means that the model will not have to learn punctuation and capatilsation, although it may have the ability to do so, this is much easier for the model to do.\n",
    "\n",
    "There are other situations where text normalisation may be used like converting digits into their text counterpart. This is not performed in this script as LibriSpeech already has the text counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28065436",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_ignore_regex = \"\".join([\",\", \"?\", \".\", \"!\", \"-\", \"\\;\", \"\\:\", \"\\\"\", \"‚Äú\", \"%\", \"‚Äò\", \"‚Äù\", \"ÔøΩ\"])\n",
    "text_column_name = \"text\"\n",
    "\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    if chars_to_ignore_regex is not None:\n",
    "        batch[\"target_text\"] = re.sub(chars_to_ignore_regex, \"\", batch[text_column_name]).lower() + \" \"\n",
    "    else:\n",
    "        batch[\"target_text\"] = batch[text_column_name].lower() + \" \"\n",
    "    return batch\n",
    "\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    remove_special_characters,\n",
    "    remove_columns=[text_column_name],\n",
    "    desc=\"remove special characters from datasets\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c580de",
   "metadata": {},
   "source": [
    "### Create Vocabulary and Tokenizer\n",
    "\n",
    "We now create a vocabulary from the dataset. This will find all the unique characters from all the normalised text in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e7e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary_from_data(\n",
    "        datasets: DatasetDict,\n",
    "        word_delimiter_token=None,\n",
    "        unk_token=None,\n",
    "        pad_token=None,\n",
    "):\n",
    "    # Given training and test labels create vocabulary\n",
    "    def extract_all_chars(batch):\n",
    "        all_text = \" \".join(batch[\"target_text\"])\n",
    "        vocab = list(set(all_text))\n",
    "        return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "    vocabs = datasets.map(\n",
    "        extract_all_chars,\n",
    "        batched=True,\n",
    "        batch_size=-1,\n",
    "        keep_in_memory=True,\n",
    "        remove_columns=datasets[\"train\"].column_names,\n",
    "    )\n",
    "\n",
    "    # take union of all unique characters in each dataset\n",
    "    vocab_set = functools.reduce(\n",
    "        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]), vocabs.values()\n",
    "    )\n",
    "\n",
    "    vocab_dict = {v: k for k, v in enumerate(sorted(list(vocab_set)))}\n",
    "\n",
    "    # replace white space with delimiter token\n",
    "    if word_delimiter_token is not None:\n",
    "        vocab_dict[word_delimiter_token] = vocab_dict[\" \"]\n",
    "        del vocab_dict[\" \"]\n",
    "\n",
    "    # add unk and pad token\n",
    "    if unk_token is not None:\n",
    "        vocab_dict[unk_token] = len(vocab_dict)\n",
    "\n",
    "    if pad_token is not None:\n",
    "        vocab_dict[pad_token] = len(vocab_dict)\n",
    "\n",
    "    return vocab_dict\n",
    "\n",
    "\n",
    "word_delimiter_token = \"|\"\n",
    "unk_token = \"[UNK]\"\n",
    "pad_token = \"[PAD]\"\n",
    "\n",
    "vocab_dict = create_vocabulary_from_data(raw_datasets,\n",
    "                                         word_delimiter_token=word_delimiter_token,\n",
    "                                         unk_token=unk_token,\n",
    "                                         pad_token=pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4541263",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20922bdc",
   "metadata": {},
   "source": [
    "With the vocabulary generated from the normalised trascripts we create a `tokenizer` which is included in the ü§ó`transformers` library. This will later be used to encode text into indexes, and decode indexes into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name_or_path = \"/tmp/wav2vec2-notebook\"\n",
    "\n",
    "vocab_file = os.path.join(tokenizer_name_or_path, \"vocab.json\")\n",
    "\n",
    "if os.path.isfile(vocab_file):\n",
    "    os.remove(vocab_file)\n",
    "\n",
    "os.makedirs(tokenizer_name_or_path, exist_ok=True)\n",
    "\n",
    "with open(vocab_file, \"w\") as file:\n",
    "    json.dump(vocab_dict, file)\n",
    "\n",
    "tokenizer_kwargs = {\n",
    "    \"config\": None,\n",
    "    \"tokenizer_type\": \"wav2vec2\",\n",
    "    \"unk_token\": unk_token,\n",
    "    \"pad_token\": pad_token,\n",
    "    \"word_delimiter_token\": word_delimiter_token,\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path, use_auth_token=False, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d06af",
   "metadata": {},
   "source": [
    "Let's look at an example for using the tokenizer. The vocabulary does not contain any digits, so these will be set to `[UNK]`. Remember, any special characters (such as commas) have already been removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82868fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"wav2vec2 finetuning on ipu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdae8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer(\"wav2vec2 finetuning on ipu\").input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bbe23",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Now we generate the feature extraction method for the model and map it across the datasets onto the audio data. In this model we are learning from raw audio signal, so the feature extraction is just used to resample the audio to the rate which the model expects. \n",
    "\n",
    "Afterwards we set the minimum and maximum input lengths in samples. These are set to 2.0 and 15.6 seconds, converted to 32000 and 249600. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f1437",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "\n",
    "dataset_sampling_rate = next(iter(raw_datasets.values())).features[\"audio\"].sampling_rate\n",
    "if dataset_sampling_rate != 16000:\n",
    "    raw_datasets = raw_datasets.cast_column(\"audio\", datasets.features.Audio(sampling_rate=16000))\n",
    "\n",
    "max_input_length = int(15.6 * feature_extractor.sampling_rate)\n",
    "min_input_length = int(2.0 * feature_extractor.sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd71404",
   "metadata": {},
   "source": [
    "### Prepare Dataset\n",
    "\n",
    "In this step both the feature extraction and tokenization are applied to the audio and transcript, respectively. The feature extractor resamples the audio, and the tokenizer will convert the normalised text into indexes.\n",
    "\n",
    "After the map function, the dataset will be filtered by the audio length. If the length of the raw audio is not between 2.0 and 15.6 seconds then it will be removed from the data. The result of filtering is cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    sample = batch[\"audio\"]\n",
    "\n",
    "    inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "\n",
    "    batch[\"labels\"] = tokenizer(batch[\"target_text\"]).input_ids\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    try:\n",
    "        return length > min_input_length and length < max_input_length\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "vectorized_datasets = raw_datasets.map(prepare_dataset,\n",
    "                                       remove_columns=raw_datasets[\"train\"].column_names,\n",
    "                                       num_proc=8,\n",
    "                                       desc=\"preprocess datasets\")\n",
    "\n",
    "vectorized_datasets = vectorized_datasets.filter(is_audio_in_length_range,\n",
    "                                                 input_columns=[\"input_length\"],\n",
    "                                                 num_proc=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcce07e",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "With the dataset prepared, the majority of the processing is complete nearly fit to be sent to the model. The role of the collator is to pad the resampled audio and encoded text to a static size. The padding values for audio will be set to `0.0` but for the indexes they will be `-100` so it's not confused with an index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.AutoProcessor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"input_values\"] = batch[\"input_values\"].half()\n",
    "\n",
    "        return batch.data\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, pad_to_multiple_of=int(max_input_length),\n",
    "                                           pad_to_multiple_of_labels=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56b494",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "\n",
    "For the model we are using `wav2vec2-base` from the HuggingFace model-hub. This model has been pretrained only.\n",
    "Some of the defaults options for the model will need to be changed for training:\n",
    "* CTC loss will be normalised by the lengths\n",
    "* There is no masking of the features to be applied so both masks are set to 0.0, the current masking strategy isn't supported on IPU.\n",
    "* The [PAD] index and vocabulary size are later used in the model for the final output layer and CTC-loss.\n",
    "* Epsilon adjusted for FP16 training.\n",
    "\n",
    "\n",
    "The IPU config describes how to parallelise the model across several IPUs. It also includes additional options such as gradient accumulation, device iterations, and memory proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "config.update(\n",
    "    {\n",
    "        \"ctc_loss_reduction\": \"mean\",\n",
    "        \"mask_time_prob\": 0.0,\n",
    "        \"mask_feature_prob\": 0.0,\n",
    "        \"layerdrop\": 0.0,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"vocab_size\": len(tokenizer),\n",
    "        \"layer_norm_eps\": 0.0001,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base\", config=config)\n",
    "\n",
    "ipu_config = IPUConfig.from_pretrained(\"Graphcore/wav2vec2-base-ipu\", executable_cache_dir=executable_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79f613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config.layers_per_ipu = [5, 5, 5, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c69b73",
   "metadata": {},
   "source": [
    "Let's set our training hyperparameters using `IPUTrainingArguments`. This subclasses the Hugging Face `TrainingArguments` class, adding parameters specific to the IPU and its execution characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c80b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = IPUTrainingArguments(output_dir= checkpoint_directory,\n",
    "                                     overwrite_output_dir=True,\n",
    "                                     do_train=True,\n",
    "                                     do_eval=True,\n",
    "                                     evaluation_strategy=\"epoch\",\n",
    "                                     learning_rate=3e-4,\n",
    "                                     num_train_epochs=5.0,\n",
    "                                     adam_epsilon=0.0001,\n",
    "                                     warmup_steps=400,\n",
    "                                     dataloader_drop_last=True,\n",
    "                                     dataloader_num_workers=16,\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a3007",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor.save_pretrained(training_args.output_dir)\n",
    "tokenizer.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7df96",
   "metadata": {},
   "source": [
    "The performance of the model is measured using the WER. This metric takes a predicted string and the correct string and computes an edit distance normalised by the length. For many sentences the sum of the edit distances is normalised by the sum of the lengths. \n",
    "\n",
    "To add this metric to our evaluation we define a `compute_metrics` function and load the metric from the `datasets` package. This is performed once after all the evaluation outputs have been computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1ba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = {\"wer\": load_metric(\"wer\")}\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = tokenizer.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    metrics = {k: v.compute(predictions=pred_str, references=label_str) for k, v in eval_metrics.items()}\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97894b6e",
   "metadata": {},
   "source": [
    "To train the model, we define a trainer using the `IPUTrainer` class which takes care of compiling the model to run on IPUs, and of performing training and evaluation. The `IPUTrainer` class works just like the HuggingFace `Trainer` class, but takes the additional `ipu_config` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f79a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = IPUTrainer(\n",
    "    model=model,\n",
    "    ipu_config=ipu_config,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"eval\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e91b6",
   "metadata": {},
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a4b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
