{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis (using IPUs)\n",
    "\n",
    "Integration of the Graphcore Intelligence Processing Unit (IPU) and the Hugging Face transformer library means that it only takes a few lines of code to perform complex tasks which require deep learning.\n",
    "\n",
    "In this notebook we perform **sentiment analysis**: we use natural language processing models to classify text prompts. \n",
    "We follow [this blog post by Federico Pascual](https://huggingface.co/blog/sentiment-analysis-python) and test 5 different models available on Hugging Face Hub to highlight different model properties of the models that can be leveraged for downstream tasks.\n",
    "\n",
    "The ease of use of the `pipeline` interface lets us quickly experiment with the pre-trained models and identify which one will work best.\n",
    "This simple interface means that it is extremely easy to access the fast inference performance of the IPU on your application.\n",
    "\n",
    "<img src=\"images/text_classification.png\" alt=\"Widget inference on a text classification task\" style=\"width:500px;\">\n",
    "\n",
    "\n",
    "While this notebook is focused on using the model (inference), our [text_classification](text_classification.ipynb) notebook will show you how to fine tune a model for a specific task using the [`datasets`](https://huggingface.co/docs/datasets/index) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's make sure your environment has the latest version of [ðŸ¤— Optimum Graphcore](https://github.com/huggingface/optimum-graphcore) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://alexandrep%40graphcore.ai:****@artifactory.sourcevertex.net:443/api/pypi/pypi-virtual/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: optimum-graphcore<0.5,>=0.4 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (0.4.2.dev0)\n",
      "Requirement already satisfied: transformers==4.20.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (4.20.1)\n",
      "Requirement already satisfied: tokenizers in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (0.12.1)\n",
      "Requirement already satisfied: scipy in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (1.9.3)\n",
      "Requirement already satisfied: sentencepiece in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (0.1.97)\n",
      "Requirement already satisfied: pillow in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (9.3.0)\n",
      "Requirement already satisfied: optimum in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (1.4.1)\n",
      "Requirement already satisfied: torch in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (1.10.0+cpu)\n",
      "Requirement already satisfied: datasets in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum-graphcore<0.5,>=0.4) (2.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (21.3)\n",
      "Requirement already satisfied: requests in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (1.23.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (0.10.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (4.64.1)\n",
      "Requirement already satisfied: filelock in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (3.8.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (2022.8.2)\n",
      "Requirement already satisfied: aiohttp in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (3.8.3)\n",
      "Requirement already satisfied: xxhash in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (3.0.0)\n",
      "Requirement already satisfied: pandas in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (1.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (9.0.0)\n",
      "Requirement already satisfied: multiprocess in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (0.70.13)\n",
      "Requirement already satisfied: dill<0.3.6 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from datasets->optimum-graphcore<0.5,>=0.4) (0.3.5.1)\n",
      "Requirement already satisfied: sympy in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum->optimum-graphcore<0.5,>=0.4) (1.11.1)\n",
      "Requirement already satisfied: coloredlogs in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from optimum->optimum-graphcore<0.5,>=0.4) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from torch->optimum-graphcore<0.5,>=0.4) (4.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from aiohttp->datasets->optimum-graphcore<0.5,>=0.4) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from requests->transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (2022.9.24)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from transformers==4.20.1->optimum-graphcore<0.5,>=0.4) (3.20.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from coloredlogs->optimum->optimum-graphcore<0.5,>=0.4) (10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore<0.5,>=0.4) (2022.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from pandas->datasets->optimum-graphcore<0.5,>=0.4) (2.8.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from sympy->optimum->optimum-graphcore<0.5,>=0.4) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets->optimum-graphcore<0.5,>=0.4) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://alexandrep%40graphcore.ai:****@artifactory.sourcevertex.net:443/api/pypi/pypi-virtual/simple, https://pypi.python.org/simple/\n",
      "Requirement already satisfied: emoji==0.6.0 in /localdata/alexandrep/sdk-envs/poplar_sdk-ubuntu_20_04-3.0.0+1145-1b114aac3a/3.0.0+1145_poptorch/lib/python3.8/site-packages (0.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"optimum-graphcore>=0.4, <0.5\"\n",
    "%pip install emoji==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"POPTORCH_LOG_LEVEL\"] = \"ERR\"\n",
    "\n",
    "executable_cache_dir = os.getenv(\"POPLAR_EXECUTABLE_CACHE_DIR\", \"/tmp/exe_cache/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using transformers pipelines on the IPU\n",
    "\n",
    "The simplest way to use a model on the IPU is to use the `pipeline` function. It provides a set of models which have been validated to work on a given task, to get started choose the task and call the `pipeline` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/localdata/alexandrep/paperspace-forks/optimum-graphcore-fork/optimum/graphcore/ipu_configuration.py:148: UserWarning: The \"enable_half_first_order_momentum\" parameter is deprecated\n",
      "  warnings.warn('The \"enable_half_first_order_momentum\" parameter is deprecated')\n",
      "/localdata/alexandrep/paperspace-forks/optimum-graphcore-fork/optimum/graphcore/ipu_configuration.py:140: UserWarning: The \"sharded_execution_for_inference\" parameter is deprecated, sharded execution is always used during inference\n",
      "  warnings.warn(\n",
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n"
     ]
    }
   ],
   "source": [
    "from optimum.graphcore import pipelines\n",
    "sentiment_pipeline = pipelines.pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline defaults to `distilbert-base-uncased-finetuned-sst-2-english`, a checkpoint managed by Hugging Face because we did not provide a specific model.\n",
    "We are helpfully warned that we should explicitly specify a maximum sequence length through the `max_length` argument if we were to put this model in production, but while we experiment we will leave it as is.\n",
    "\n",
    "Now it's time to test our first prompts. Let's start with some very easy to classify text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998660087585449},\n",
       " {'label': 'NEGATIVE', 'score': 0.9990818500518799}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test_data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reassuringly, the model got it right! And with a high degree of confidence, more than 99.9% in both cases.\n",
    "\n",
    "The first call to the pipeline was a bit slow, it took several seconds to provide the answer. This behaviour is due to compilation of the model which happens on the first call.\n",
    "On subsequent prompts it is much faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998660087585449},\n",
       " {'label': 'NEGATIVE', 'score': 0.9990818500518799}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that was much faster! We can use the `%%timeit` cell magic to check how fast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.68 ms Â± 76.3 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "sentiment_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes on the order of ~1ms per prompt, this is fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other tasks supported by IPU pipelines\n",
    "\n",
    "This simple interface provides access to a number of other tasks which can be listed through the `list_tasks` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio-classification',\n",
       " 'automatic-speech-recognition',\n",
       " 'fill-mask',\n",
       " 'image-classification',\n",
       " 'ner',\n",
       " 'question-answering',\n",
       " 'sentiment-analysis',\n",
       " 'text-classification',\n",
       " 'token-classification',\n",
       " 'zero-shot-classification']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pipelines.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customising the IPU resources\n",
    "\n",
    "Depending on your system you may have 4, 16 or 64 IPUs available. IPUs are designed from the ground up to make it easy to scale applications to large numbers of processors working together, however, in this case it is not needed, 1 IPU is sufficient.\n",
    "\n",
    "We're going to make sure that we are using a single IPU so that other users or other applications that we are running in the background are not affected. To do that, we define an `inference_config` dictionary which contains arguments that will be passed to the `optimum.graphcore.IPUConfig` object which controls the accelerator. \n",
    "\n",
    "We recreate our pipeline with the new settings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998660087585449},\n",
       " {'label': 'NEGATIVE', 'score': 0.9990818500518799}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_config = dict(layers_per_ipu=[40], ipus_per_replica=1, enable_half_partials=True,\n",
    "                        matmul_proportion=0.6, executable_cache_dir=executable_cache_dir)\n",
    "sentiment_pipeline = pipelines.pipeline(\"sentiment-analysis\", ipu_config_kwargs=inference_config)\n",
    "data = [\"I love you\", \"I hate you\"]\n",
    "sentiment_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It still works as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking more complex questions\n",
    "\n",
    "Now, our initial prompts are trivial to classify, what if we asked the pipeline to classify a more ambiguous sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_prompts = [\n",
    "    \"How are you today?\", \n",
    "    \"I'm a little tired, I didn't sleep well, but I hope it gets better\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sentence is perfectly neutral, while the second is a mix of negative and positive sentiment.\n",
    "A good answer from the model would reflect the ambiguous nature of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9356999397277832},\n",
       " {'label': 'POSITIVE', 'score': 0.9859092831611633}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline(ambiguous_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only supports two labels: \"POSITIVE\" and \"NEGATIVE\"; neither of which really captures the sentiment of those messages.\n",
    "While we see a slight drop in the confidence of the model, but it does not feel sufficient to reflect message.\n",
    "\n",
    "The imprecise classification of these prompts would affect any downstream task: if we were trying to derive some insights from the model on customer satisfaction we may have an overly optimistic view of performance and derive the wrong conclusions.\n",
    "\n",
    "To resolve this issue we need to try more models, which accommodate finer grained classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying more models\n",
    "\n",
    "[The blog post](https://huggingface.co/blog/sentiment-analysis-python) we are following suggests a number of other models, lets try them all to see if they perform better on our ambiguous prompts!\n",
    "\n",
    "The first one is `finiteautomata/bertweet-base-sentiment-analysis` a [RoBERTa model trained on 40 thousand tweets](https://huggingface.co/finiteautomata/bertweet-base-sentiment-analysis) collected before 2018. Using it is as simple as giving the name of the ðŸ¤— hub repository as the model argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you', 'I hate you']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9902849793434143},\n",
       " {'label': 'NEG', 'score': 0.979720413684845}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet18_pipeline = pipelines.pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"finiteautomata/bertweet-base-sentiment-analysis\", ipu_config_kwargs=inference_config,\n",
    ")\n",
    "print(simple_test_data)\n",
    "tweet18_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the model correctly classifies our simple prompt. Now let's try our more ambiguous prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEU', 'score': 0.7505843043327332},\n",
       " {'label': 'NEG', 'score': 0.8974782228469849}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet18_pipeline([\"How are you today?\", \"I'm a little tired, I didn't sleep well, but I hope it gets better\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is much better: the model classifies the first prompt as neutral and the second as negative. The addition of a \"NEU\" (neutral) class gives the model the flexibility to correctly identify statements which do not fit as positive or negative.\n",
    "\n",
    "The challenge of the second prompt is that it has multiple clauses which capture different sentiments. To get a better result on it you might separate it out into multiple prompts that are better suited to the model. For example we can split on `,` to classify each clause of the sentence on its own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEU', 'score': 0.7505843043327332},\n",
       " {'label': 'NEG', 'score': 0.9528174996376038},\n",
       " {'label': 'NEG', 'score': 0.9658814668655396},\n",
       " {'label': 'POS', 'score': 0.8587332367897034}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_prompts_answer = tweet18_pipeline([\n",
    "    \"How are you today?\", \n",
    "    *\"I'm a little tired, I didn't sleep well, but I hope it gets better\".split(\",\"),\n",
    "])\n",
    "split_prompts_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here each parts of the sentence is correctly classified, how you choose to process those sentence parts will depend on what you need to use the results of the sentiment analysis for.\n",
    "\n",
    "We can do small changes to the prompt to get an intuition of how the model responds to changes in grammar. Below the last part of the ambiguous sentence is changed to be more optimistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous score: {'label': 'POS', 'score': 0.8587332367897034}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9822455644607544}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Previous score: {split_prompts_answer[-1]}\")\n",
    "\n",
    "tweet18_pipeline([\"but it is getting better\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence of that change the score associated with the positive label has gone up, matching the desired behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A model finetuned on tweets\n",
    "\n",
    "The next model discussed in the blog post is the [`cardiffnlp/twitter-roberta-base-sentiment-latest`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) model, it is a RoBERTa-Base which was trained on 124M tweets collected between 2018 and 2021. This data makes the model much more recent than the previous pre-trained checkpoint.\n",
    "\n",
    "As before this model is trivial to load through the pipeline API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9594b25c1f44c7b103b42ded523118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:02<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I love you', {'label': 'positive', 'score': 0.8565994501113892}),\n",
      " ('I hate you', {'label': 'negative', 'score': 0.7821184396743774}),\n",
      " ('How are you today?', {'label': 'neutral', 'score': 0.8373913168907166}),\n",
      " (\"I'm a little tired, I didn't sleep well, but I hope it gets better\",\n",
      "  {'label': 'negative', 'score': 0.6537167429924011})]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "tweet21_pipeline = pipelines.pipeline(\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", ipu_config_kwargs=inference_config\n",
    ")\n",
    "out = tweet21_pipeline(simple_test_data + ambiguous_prompts)\n",
    "# print prompts and predicted labels side by side\n",
    "pprint([*zip(simple_test_data + ambiguous_prompts, out)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs similarly on these prompts as the model explored in the previous section. Differences in the model may not be apparent until we start prompting about recent events.\n",
    "\n",
    "If we ask the previous model to classify a statement about the Coronavirus pandemic we get different results between the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Older model score: [{'label': 'NEU', 'score': 0.5270748138427734}]\n",
      "Newer model score: [{'label': 'negative', 'score': 0.7121769785881042}]\n"
     ]
    }
   ],
   "source": [
    "coronavirus_prompt = [\"Coronavirus is increasing\"]\n",
    "old = tweet18_pipeline(coronavirus_prompt)\n",
    "new = tweet21_pipeline(coronavirus_prompt)\n",
    "print(f\"Older model score: {old}\")\n",
    "print(f\"Newer model score: {new}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newer model has a strong negative connotation for Coronavirus while the older model sees it as a neutral statement. This simple experiment shows the importance of testing and fine-tuning models regularly to make sure that sentiment analysis continues to be accurate as connotations of certain words evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done using these pipelines in the rest of the notebook so we detach from the IPU devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gc-monitor --no-card-info | grep ${os.getpid()}\n",
    "tweet18_pipeline.model.detachFromDevice()\n",
    "tweet21_pipeline.model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will allow us to test additional models, for more details on managing IPU resources from a notebook you can consult our [notebook on managing IPU resources](managing_ipu_resources.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-lingual model\n",
    "\n",
    "The next model has an interesting feature: it is multi-lingual. It was trained on a dataset of English, Dutch, German, Spanish, Italian and French text, it can be prompted in any of these languages and should correctly classify the text inputs.\n",
    "\n",
    "The model is `nlptown/bert-base-multilingual-uncased-sentiment` a BERT checkpoint fine-tuned on ~700k reviews in 6 languages, which gives a rating between 1 and 5 stars for each prompt. 1 start indicates a very negative sentiment, while 5 starts corresponds to a very positive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb108d33fbe476291ac536d0bcfc657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love you', 'I hate you']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:03<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.853151261806488},\n",
       " {'label': '1 star', 'score': 0.6354780197143555}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "multilingual_pipeline = pipelines.pipeline(\n",
    "    model=model_name, ipu_config_kwargs=inference_config\n",
    ")\n",
    "print(simple_test_data)\n",
    "multilingual_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It successfully classifies our simple input, now let's see how it fares with our ambiguous input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.5348031520843506},\n",
       " {'label': '3 stars', 'score': 0.7582475543022156}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilingual_pipeline([\n",
    "    \"How are you today?\",  # How are you today?\n",
    "    \"I'm a little tired, I didn't sleep well, but I hope it gets better\"\n",
    "    # \"I'm a little tired, I didn't sleep well, but I hope it gets better\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is a bit optimistic about our first prompt, it's guess is given with a fairly low confidence, and it identifies the second prompt as neutral with a median score of 3.\n",
    "\n",
    "Now let's translate our prompts and ask it the same questions in French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.3300594091415405},\n",
       " {'label': '3 stars', 'score': 0.7263907790184021}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ambiguous_in_french = [\n",
    "    \"Comment vas-tu aujourd'hui?\",\n",
    "    \"Je suis un peu fatigue, je n'ai pas bien dormi mais j'espere que la journee s'ameliore\",\n",
    "]\n",
    "multilingual_pipeline(ambiguous_in_french)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives very similar results in both languages!\n",
    "\n",
    "Now let us revisit our first model, could it also work with this multi-lingual input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9356999397277832}, {'label': 'POSITIVE', 'score': 0.9859092831611633}]\n",
      "[{'label': 'NEGATIVE', 'score': 0.9645559191703796}, {'label': 'NEGATIVE', 'score': 0.9287972450256348}]\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_pipeline(ambiguous_prompts))\n",
    "print(sentiment_pipeline(ambiguous_in_french))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly it cannot, its prediction are not stable across the two languages. However this model did not perform very well on the ambiguous prompts. If we re-use some of the more precise English-only models and run them on more obvious prompts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9163401126861572}, {'label': 'neutral', 'score': 0.7056311964988708}]\n",
      "[{'label': '5 stars', 'score': 0.7591118216514587}, {'label': '1 star', 'score': 0.86994868516922}]\n"
     ]
    }
   ],
   "source": [
    "simple_french_input = [\n",
    "    \"Ce film est excellent\",  # This film is excellent\n",
    "    \"Le produit ne marche pas du tout\"  # The tool does not work at all\n",
    "]\n",
    "print(tweet21_pipeline(simple_french_input))\n",
    "print(multilingual_pipeline(simple_french_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we see the multi-lingual model correctly predict strongly positive and negative labels, while the other model predicts a positive message (correct) and a neutral (expected LABEL_0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models\n",
    "\n",
    "Models can be finetuned to extract different classes from text. The `bhadresh-savani/distilbert-base-uncased-emotion` checkpoint is a DistilBERT checkpoint tuned to identify the emotion associated with a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3418458b32e475ba9f79b0b47d85bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:01<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'love', 'score': 0.9584758281707764},\n",
       " {'label': 'anger', 'score': 0.8243763446807861}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
    "emotion_pipeline = pipelines.pipeline(model=model_name, ipu_config_kwargs=inference_config)\n",
    "emotion_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be prompted with sentences which include different emotions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'joy', 'score': 0.7177484035491943},\n",
       " {'label': 'anger', 'score': 0.9975298047065735},\n",
       " {'label': 'sadness', 'score': 0.3289742171764374},\n",
       " {'label': 'fear', 'score': 0.6390519142150879}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_pipeline([\n",
    "    \"How are you today?\", \n",
    "    \"Don't make me go out, it's too cold!\",\n",
    "    \"What is happening, I don't understand\",\n",
    "    \"Where did you come from?\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We detach from the remaining pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline.model.detachFromDevice()\n",
    "emotion_pipeline.model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a larger model\n",
    "\n",
    "The optimum library supports several sizes of models for many of the standard architectures, in this section we load a checkpoint which uses roBERTa large to perform the same task.\n",
    "\n",
    "Larger models will take longer to execute but may provide better predictions in a broader range of cases. As an example we load the [`j-hartmann/sentiment-roberta-large-english-3-classes`](https://huggingface.co/j-hartmann/sentiment-roberta-large-english-3-classes) checkpoint which uses the roBERTa-Large model trained on 5000 manually annotated social media posts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at j-hartmann/sentiment-roberta-large-english-3-classes were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No padding arguments specified, so pad to 128 by default. Inputs longer than 128 will be truncated. To change this behaviour, pass the `padding='max_length'` and`max_length=<your desired input length>` arguments to the pipeline function\n",
      "Graph compilation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:07<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9996185302734375},\n",
       " {'label': 'negative', 'score': 0.999029278755188}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_pipeline = pipelines.pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    \"j-hartmann/sentiment-roberta-large-english-3-classes\",\n",
    "    ipu_config_kwargs=inference_config\n",
    ")\n",
    "larger_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before the it succeeds on our simple example. We can check the latency of that model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.98 ms Â± 148 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "larger_pipeline(simple_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes about 4ms to execute per prompt, as expected this is slower than the earlier pipelines which used a smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neutral', 'score': 0.9991235136985779},\n",
       " {'label': 'negative', 'score': 0.9979640245437622}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "larger_pipeline([\"How are you today?\", \"I'm a little tired, I didn't sleep well, but I hope it gets better\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This notebook has followed the steps of a blog post, showing how easy using IPUs for sentiment analysis is.\n",
    "The integration with the pipeline interface makes thousands of models available on the Hugging Face hub easy to use on the IPU, while following the blog post we experimented with 6 different checkpoints testing the properties of the different models.\n",
    "\n",
    "There are [hundreds more available on the Hub](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment), try them out, we think they'll work but, if you hit an error, [raise an issue or open a pull request](https://github.com/huggingface/optimum-graphcore/issues) and we'll do our best to fix it! ðŸ¤—\n",
    "If you have your own dataset, the [text classfication notebook](text_classification.ipynb) will show you how a simple way to fine-tune a classification model tailored to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to check out one of the other tasks available through the `pipeline` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio-classification',\n",
       " 'automatic-speech-recognition',\n",
       " 'fill-mask',\n",
       " 'image-classification',\n",
       " 'ner',\n",
       " 'question-answering',\n",
       " 'sentiment-analysis',\n",
       " 'text-classification',\n",
       " 'token-classification',\n",
       " 'zero-shot-classification']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelines.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Explore another approach to sentiment analysis through text-generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('3.0.0+1145_poptorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46bde714a99d715eba7e507975e678b0968e7177d805932276a51e552e29fed0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
