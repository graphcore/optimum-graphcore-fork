{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f588db-977d-487c-aae5-1f6a76ff5376",
   "metadata": {},
   "source": [
    "# General-purpose text embeddings with E5-Large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dcd25-64d4-4380-b401-3478d41074a7",
   "metadata": {},
   "source": [
    "This notebook describes how to use the [E5 model](https://arxiv.org/pdf/2212.03533.pdf) (Emb**E**ddings from\n",
    "bidir**E**ctional **E**ncoder r**E**presentations) to generate text embeddings on the IPU. This [state-of-the-art](https://syncedreview.com/2022/12/13/microsofts-e5-text-embedding-model-tops-the-mteb-benchmark-with-40x-fewer-parameters/)  text embeddings model can be used for general purpose text embeddings for any tasks requiring a single-vector representation of texts, including retrieval, clustering and classification. The model provides general-purpose checkpoints trained without labels (unsupervised) and fine-tuned checkpoints.\n",
    "\n",
    "Here, we demonstrate how to use the fine-tuned E5 large checkpoint for inference over 4 IPUs. The checkpoint (`model_name`) can be directly modified to use one of the [unsupervised](https://github.com/microsoft/unilm/tree/master/e5#english-pre-trained-models) checkpoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d591cc4-38ca-47ae-a52f-165b04deb0a8",
   "metadata": {},
   "source": [
    "First, import the general requirements for running this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb86be2-6ba9-48dc-a948-6009dcf617e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import poptorch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c52bb-63fc-4c3d-9ee0-40ae4a1c02f5",
   "metadata": {},
   "source": [
    "We need to instatiate some global parameters that we will use to run the model. Here, we define the model name (the checkpoint which will be downloaded from the Hugging Face Hub) and the micro batch size. The micro batch size is set to 1, as we use on-device loops (`device iterations` specific to the IPU) to set a effective batch size of 32. A random seed is also set for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e503c3df-45ce-4613-ac7c-3b38a48ed40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "model_name = 'intfloat/e5-large'\n",
    "\n",
    "micro_batch_size = 1\n",
    "\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e858ca85-84a2-4fe4-8c4f-0429a8b04282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: POPLAR_ENGINE_OPTIONS={\"autoReport.all\":\"true\", \"autoReport.directory\":\"./e5-l-prof\"}\n"
     ]
    }
   ],
   "source": [
    "%env POPLAR_ENGINE_OPTIONS={\"autoReport.all\":\"true\", \"autoReport.directory\":\"./e5-l-prof\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315eb4c-84ea-49a5-a7d8-635248e22044",
   "metadata": {},
   "source": [
    "Next, define the `transformers` `AutoTokenizer` to instantiate a vocabulary tokenizer for our input text, for the task we define an maximum input sequence length of 512 and pad each sequence to the maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b32b404-fe23-4c10-a95e-433505a210b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BatchEncoding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def transform_func(example) -> BatchEncoding:\n",
    "    return tokenizer(\n",
    "        example['input_texts'],\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf200e-2375-4d58-aa41-d64daf8017ad",
   "metadata": {},
   "source": [
    "We define some IPU specific configurations to get the most out of the model. A different configuration is described here for the smaller checkpoint `e5-small` as it can run directly on a single IPU. The larger model is pipelined over 4 IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e140bc-d2e2-4da3-a53d-3532094554fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`replicated_tensor_sharding` is not used when `replication_factor=1`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fbdb5e7def0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.graphcore import IPUConfig\n",
    "\n",
    "ipu_configs = {}\n",
    "\n",
    "ipu_configs['intfloat/e5-small'] = {\n",
    "    \"inference_device_iterations\": 32,\n",
    "    \"inference_replication_factor\": 1,\n",
    "    \"executable_cache_dir\": \"./exe_cache\",\n",
    "    \"matmul_proportion\": 0.5,\n",
    "    \"replicated_tensor_sharding\": True,\n",
    "    \"ipus_per_replica\": 1,\n",
    "    \"profile_dir\": \"e5-small-profile\"\n",
    "}\n",
    "\n",
    "ipu_configs['intfloat/e5-small-unsupervised'] = ipu_configs['intfloat/e5-small']\n",
    "\n",
    "ipu_configs['intfloat/e5-large'] = {\n",
    "    \"embedding_serialization_factor\": 2,\n",
    "    \"enable_half_partials\": True,\n",
    "    \"executable_cache_dir\": \"./exe_cache\",\n",
    "    \"inference_device_iterations\": 32,\n",
    "    \"inference_replication_factor\": 1,\n",
    "    \"ipus_per_replica\": 4,\n",
    "    \"layers_per_ipu\": [3, 7, 7, 7],\n",
    "    \"matmul_proportion\": [0.1, 0.15, 0.15, 0.15],\n",
    "    \"profile_dir\": \"e5-large-profile\",\n",
    "    \"recompute_checkpoint_every_layer\": True,\n",
    "    \"replicated_tensor_sharding\": True,\n",
    "}\n",
    "\n",
    "ipu_configs['intfloat/e5-large-unsupervised'] = ipu_configs['intfloat/e5-large']\n",
    "\n",
    "ipu_config = IPUConfig.from_dict(ipu_configs[model_name]).eval()\n",
    "ipu_opts = ipu_config.to_options(for_inference=True)\n",
    "\n",
    "ipu_opts.randomSeed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b40f13-ccc2-423f-b2f3-b23867dae7d7",
   "metadata": {},
   "source": [
    "The model config needs to be instantiated for the E5 model. E5 uses a bidirectional encoder, essentially the encoder stage of a BERT model, to generate the trained embeddings. The config will define the architecture of the model, such as the number of encoder layers and size of the hidden dimension within the model.\n",
    "\n",
    "The larger E5 model is run over 4 IPUs, to do this, we use IPU pipeline parallelism - the stages of the model run on each IPU used are defined by the `PipelinedE5Model` class from `modeling_e5.py` which subclasses the BERT encoder and uses the `parallelize()` function to define the device information and stage for each set of layers in the model.\n",
    "\n",
    "To run the model on the IPU, we simply need to import the `PipelinedE5Model`, pass the pretrained config to it and define the custom IPU config for the model, as certain parameters in the IPU config are used within the parallelisation function.\n",
    "\n",
    "Finally, the model is passed into a `poptorch.inferenceModel()` wrapper to create an IPU-ready executor for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8005d4c-80e1-4f21-96db-0293d9868de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from modeling_e5 import PipelinedE5Model\n",
    "\n",
    "e5_config = AutoConfig.from_pretrained(model_name)\n",
    "e5_model = PipelinedE5Model.from_pretrained(model_name, config=e5_config)\n",
    "e5_model.ipu_config = ipu_config\n",
    "\n",
    "e5_model_ipu = poptorch.inferenceModel(e5_model.parallelize(), ipu_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec397618-4bfa-45b9-bbc1-dc3138dc725d",
   "metadata": {},
   "source": [
    "Here, we create a dummy dataset for the model. Using the Hugging Face `datasets` library we can create the dataset from a simple dictionary of `'input_texts'` and use the `map()` method to tokenize each of the inputs of the dataset.\n",
    "\n",
    "Finally, we can convert the Hugging Face Arrow format dataset to a Pytorch ready dataset with `set_format` which converts the tokenized inputs into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591e6055-2e37-41b0-891b-dda98e449513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "examples = {}\n",
    "examples['input_texts'] = [\"Some cats don't learn how to eat solid foods till they are five years old.\"] * 1024\n",
    "dataset: Dataset = Dataset.from_dict(examples)\n",
    "\n",
    "dataset = dataset.map(transform_func, batched=True)\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d4c56-6272-4344-98cc-bd725b19a7ef",
   "metadata": {},
   "source": [
    "The tokenized dataset is passed to the [`poptorch.Dataloader`](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html) to create a IPU-ready batched dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aadfc38-12fd-4b70-9466-478025b33454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator as data_collator\n",
    "\n",
    "poptorch_dataloader = poptorch.DataLoader(\n",
    "    ipu_opts,\n",
    "    dataset,\n",
    "    batch_size=micro_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=data_collator\n",
    "#   mode=poptorch.DataLoaderMode.Async\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a5e6a-e4d4-4d8b-b3a7-25a59047f674",
   "metadata": {},
   "source": [
    "We define a simple `infer()` function which will perform inference iteratively on each batch and return the concatenated list of embeddings for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d94eda93-6954-4cd8-965d-594795782b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, dataloader):\n",
    "    encoded_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_dict in tqdm(dataloader, desc='encoding'):\n",
    "            lat = time.time()\n",
    "            \n",
    "            outputs = model(**batch_dict)\n",
    "            encoded_embeds.append(outputs.cpu().numpy())\n",
    "            \n",
    "            lat = time.time() - lat\n",
    "            print(f\"batch latency: {lat}s | per_sample: {lat/len(batch_dict['input_ids'])}\")\n",
    "    \n",
    "    return np.concatenate(encoded_embeds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c2f53-0aca-4e89-8db8-09e9a1795061",
   "metadata": {},
   "source": [
    "To run the model, first we pass an arbitrary call to the model using the first batch to ensure we have compiled the model executable (or loaded the already compiled executable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77fa8c3-e477-4ebe-9dcd-a35730f4b81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:46<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile time: 212.9538974761963\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "c = time.time()\n",
    "e5_model_ipu(**next(iter(poptorch_dataloader)))\n",
    "print(f\"Compile time: {time.time() - c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacec9c0-1a24-4db7-938b-34fd28864a2c",
   "metadata": {},
   "source": [
    "Then, simply call the infer function to generate embeddings for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9d4bb7-cfec-4251-8fe4-1d51c9873b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b419a18dbda2468991db79514aff86a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch latency: 0.4789144992828369s | per_sample: 0.014966078102588654\n",
      "batch latency: 0.4763667583465576s | per_sample: 0.014886461198329926\n",
      "batch latency: 0.480635404586792s | per_sample: 0.01501985639333725\n",
      "batch latency: 0.4762880802154541s | per_sample: 0.01488400250673294\n",
      "batch latency: 0.476625919342041s | per_sample: 0.014894559979438782\n",
      "batch latency: 0.47629594802856445s | per_sample: 0.01488424837589264\n",
      "batch latency: 0.4761989116668701s | per_sample: 0.014881215989589691\n",
      "batch latency: 0.4761807918548584s | per_sample: 0.014880649745464325\n",
      "batch latency: 0.4760255813598633s | per_sample: 0.014875799417495728\n",
      "batch latency: 0.4758172035217285s | per_sample: 0.014869287610054016\n",
      "batch latency: 0.4763762950897217s | per_sample: 0.014886759221553802\n",
      "batch latency: 0.4763514995574951s | per_sample: 0.014885984361171722\n",
      "batch latency: 0.4764840602874756s | per_sample: 0.014890126883983612\n",
      "batch latency: 0.4766852855682373s | per_sample: 0.014896415174007416\n",
      "batch latency: 0.47601866722106934s | per_sample: 0.014875583350658417\n",
      "batch latency: 0.4757273197174072s | per_sample: 0.014866478741168976\n",
      "batch latency: 0.47581028938293457s | per_sample: 0.014869071543216705\n",
      "batch latency: 0.4768095016479492s | per_sample: 0.014900296926498413\n",
      "batch latency: 0.47649383544921875s | per_sample: 0.014890432357788086\n",
      "batch latency: 0.4785270690917969s | per_sample: 0.014953970909118652\n",
      "batch latency: 0.47774553298950195s | per_sample: 0.014929547905921936\n",
      "batch latency: 0.4765949249267578s | per_sample: 0.014893591403961182\n",
      "batch latency: 0.47658300399780273s | per_sample: 0.014893218874931335\n",
      "batch latency: 0.47650837898254395s | per_sample: 0.014890886843204498\n",
      "batch latency: 0.4763917922973633s | per_sample: 0.014887243509292603\n",
      "batch latency: 0.4776899814605713s | per_sample: 0.014927811920642853\n",
      "batch latency: 0.4768238067626953s | per_sample: 0.014900743961334229\n",
      "batch latency: 0.4760267734527588s | per_sample: 0.014875836670398712\n",
      "batch latency: 0.47600340843200684s | per_sample: 0.014875106513500214\n",
      "batch latency: 0.47632718086242676s | per_sample: 0.014885224401950836\n",
      "batch latency: 0.47688770294189453s | per_sample: 0.014902740716934204\n",
      "batch latency: 0.4767944812774658s | per_sample: 0.014899827539920807\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "embeddings = infer(e5_model_ipu, poptorch_dataloader)\n",
    "runtime = time.time() - runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903931f0-bb08-472a-a33d-6ff29b773a9e",
   "metadata": {},
   "source": [
    "Finally, lets print out one of the results, and the total IPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b170bf1a-1db6-4d8b-9099-479eb616bb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPU runtime: 32.74954581260681\n",
      " First embedding: [-0.03733141 -0.09052704  0.02737014 ... -0.01642533  0.0154235\n",
      " -0.00438281]\n",
      " Shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"IPU runtime: {runtime}\\n First embedding: {embeddings[0]}\\n Shape: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4393d7-8706-4d2d-99a1-fe4cdd41e06a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
