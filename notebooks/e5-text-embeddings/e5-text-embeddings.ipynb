{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f588db-977d-487c-aae5-1f6a76ff5376",
   "metadata": {},
   "source": [
    "# General-purpose text embeddings with E5-Large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781dcd25-64d4-4380-b401-3478d41074a7",
   "metadata": {},
   "source": [
    "This notebook describes how to use the [E5 model](https://arxiv.org/pdf/2212.03533.pdf) (Emb**E**ddings from\n",
    "bidir**E**ctional **E**ncoder r**E**presentations) to generate text embeddings on the IPU. This [state-of-the-art](https://syncedreview.com/2022/12/13/microsofts-e5-text-embedding-model-tops-the-mteb-benchmark-with-40x-fewer-parameters/)  text embeddings model can be used for general purpose text embeddings for any tasks requiring a single-vector representation of texts, including embeddings retrieval and semantic search, clustering and classification. The E5 model provides general-purpose checkpoints trained without labels (unsupervised) and fine-tuned checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9ee64-7372-467f-9c9b-bbf53477ed2b",
   "metadata": {},
   "source": [
    "!['E5 Model'](images/2Dretrieval.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a0eeb-dc30-49f9-9340-93881810fb43",
   "metadata": {},
   "source": [
    "Here, we demonstrate how to use the fine-tuned E5 large checkpoint for inference over 4 IPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d591cc4-38ca-47ae-a52f-165b04deb0a8",
   "metadata": {},
   "source": [
    "First, install the requirements for running this notebook. This will install `gradio` and `sentence_transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3bbd1-07d4-4bc6-8521-e690d728666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a8017-830e-45fd-9a14-76661bd61424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2caff0d-c9dd-426b-a62f-08e53ff1c201",
   "metadata": {},
   "source": [
    "Next, import the general required modules for the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb86be2-6ba9-48dc-a948-6009dcf617e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import poptorch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885c52bb-63fc-4c3d-9ee0-40ae4a1c02f5",
   "metadata": {},
   "source": [
    "We need to instatiate some global parameters that we will use to run the model. Here, we define the model name (the checkpoint which will be downloaded from the Hugging Face Hub) and the micro batch size. The micro batch size is set to 1, as we use on-device loops (`device iterations` specific to the IPU) to set a effective batch size of 32. A random seed is also set for reproducibility.\n",
    "\n",
    "The checkpoint (`model_name`) can be directly modified to use one of the [unsupervised](https://github.com/microsoft/unilm/tree/master/e5#english-pre-trained-models) checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e503c3df-45ce-4613-ac7c-3b38a48ed40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"\")\n",
    "\n",
    "model_name = 'intfloat/e5-large'\n",
    "pod_type = os.getenv(\"GRAPHCORE_POD_TYPE\", \"pod4\")\n",
    "\n",
    "n_ipu = 1\n",
    "micro_batch_size = 2\n",
    "device_iterations = 256\n",
    "replication_factor = None\n",
    "\n",
    "max_seq_len = 512\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315eb4c-84ea-49a5-a7d8-635248e22044",
   "metadata": {},
   "source": [
    "Next, define the `transformers` `AutoTokenizer` to instantiate a vocabulary tokenizer for our input text, for the task we define an maximum input sequence length of 512 and pad each sequence to the maximum sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b32b404-fe23-4c10-a95e-433505a210b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BatchEncoding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def transform_func(example) -> BatchEncoding:\n",
    "    return tokenizer(\n",
    "        example['text'],\n",
    "        max_length=max_seq_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf200e-2375-4d58-aa41-d64daf8017ad",
   "metadata": {},
   "source": [
    "We define some IPU specific configurations to get the most out of the model. A different configuration is described here for the smaller checkpoint `e5-small` as it can run directly on a single IPU. The larger model is pipelined over 4 IPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e140bc-d2e2-4da3-a53d-3532094554fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localdata/arsalanu/popsdk_venvs/3.2.1+1370/3.2.1+1370_poptorch/lib/python3.8/site-packages/optimum/graphcore/ipu_configuration.py:403: UserWarning: The \"enable_half_first_order_momentum\" parameter is deprecated\n",
      "  warnings.warn('The \"enable_half_first_order_momentum\" parameter is deprecated')\n"
     ]
    }
   ],
   "source": [
    "from config import get_ipu_config\n",
    "\n",
    "ipu_config = get_ipu_config(pod_type, n_ipu, device_iterations, replication_factor, random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b40f13-ccc2-423f-b2f3-b23867dae7d7",
   "metadata": {},
   "source": [
    "The model config needs to be instantiated for the E5 model. E5 uses a bidirectional encoder, essentially the encoder stage of a BERT model, to generate the trained embeddings. The config will define the architecture of the model, such as the number of encoder layers and size of the hidden dimension within the model.\n",
    "\n",
    "The larger E5 model is run over 4 IPUs, to do this, we use IPU pipeline parallelism - the stages of the model run on each IPU used are defined by the `PipelinedE5Model` class from `modeling_e5.py` which subclasses the BERT encoder and uses the `parallelize()` function to define the device information and stage for each set of layers in the model.\n",
    "\n",
    "To run the model on the IPU, we simply need to import the `PipelinedE5Model`, pass the pretrained config to it and define the custom IPU config for the model, as certain parameters in the IPU config are used within the parallelisation function.\n",
    "\n",
    "Finally, the model is passed into a `poptorch.inferenceModel()` wrapper to create an IPU-ready executor for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8005d4c-80e1-4f21-96db-0293d9868de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "from modeling_e5 import PipelinedE5Model\n",
    "\n",
    "from optimum.graphcore.modeling_utils import to_pipelined\n",
    "\n",
    "e5_config = AutoConfig.from_pretrained(model_name)\n",
    "e5_model = PipelinedE5Model.from_pretrained(model_name, config=e5_config).eval().half()\n",
    "e5_model.ipu_config = ipu_config\n",
    "\n",
    "ipu_options = ipu_config.to_options(for_inference=True)\n",
    "e5_model_ipu = poptorch.inferenceModel(e5_model.parallelize(), ipu_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec397618-4bfa-45b9-bbc1-dc3138dc725d",
   "metadata": {},
   "source": [
    "Lets load a dataset to try out the model. Using the Hugging Face `datasets` library we can load a pre-existing dataset from the Hugging Face Hub. In this case, lets use the `go_emotions` dataset. Later in the notebook, we will use this dataset to perform create a basic semantic search functionality.\n",
    "\n",
    "The dataset first needs to be tokenized, we can use the `map()` method to tokenize each of the inputs of the dataset.\n",
    "\n",
    "Finally, we can convert the Hugging Face Arrow format dataset to a Pytorch ready dataset with `set_format` which converts the tokenized inputs into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "591e6055-2e37-41b0-891b-dda98e449513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e50a823f804a44958dba1a5e59ac16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-0b4c773d2d3df4ba.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-2872aae7edfd2479.arrow\n",
      "Loading cached processed dataset at /home/arsalanu/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-231657c4476b8f66.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "\n",
    "tokenized_dataset = dataset.map(transform_func, batched=True)\n",
    "\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d4c56-6272-4344-98cc-bd725b19a7ef",
   "metadata": {},
   "source": [
    "The tokenized dataset is passed to the [`poptorch.Dataloader`](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/batching.html) to create a IPU-ready batched dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aadfc38-12fd-4b70-9466-478025b33454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator as data_collator\n",
    "\n",
    "poptorch_dataloader = poptorch.DataLoader(\n",
    "    ipu_options,\n",
    "    tokenized_dataset['train'],\n",
    "    batch_size=micro_batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747a5e6a-e4d4-4d8b-b3a7-25a59047f674",
   "metadata": {},
   "source": [
    "We define a simple `infer()` function which will perform inference iteratively on each batch and return the concatenated list of embeddings for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94eda93-6954-4cd8-965d-594795782b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, dataloader):\n",
    "    encoded_embeds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_dict in tqdm(dataloader, desc='encoding'):\n",
    "            lat = time.time()\n",
    "            outputs = model(**batch_dict)\n",
    "            lat = time.time() - lat\n",
    "            \n",
    "            encoded_embeds.append(outputs)\n",
    "            print(f\"batch len: {len(batch_dict['input_ids'])} | batch latency: {lat}s | per_sample: {lat/len(batch_dict['input_ids'])}s | throughput: {len(batch_dict['input_ids'])/lat} samples/s\")\n",
    "    \n",
    "    return torch.cat(encoded_embeds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c2f53-0aca-4e89-8db8-09e9a1795061",
   "metadata": {},
   "source": [
    "To run the model, first we pass an arbitrary call to the model using the first batch to ensure we have compiled the model executable (or loaded the already compiled executable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d77fa8c3-e477-4ebe-9dcd-a35730f4b81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00]\n",
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile time: 29.782036304473877\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "c = time.time()\n",
    "e5_model_ipu(**next(iter(poptorch_dataloader)))\n",
    "print(f\"Compile time: {time.time() - c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacec9c0-1a24-4db7-938b-34fd28864a2c",
   "metadata": {},
   "source": [
    "Then, simply call the `infer` function to generate embeddings for the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c9d4bb7-cfec-4251-8fe4-1d51c9873b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0c0c10624c4c17ae68bb231adb2d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "encoding:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch len: 2048 | batch latency: 2.3720037937164307s | per_sample: 0.001158204977400601s | throughput: 863.4050271864089 samples/s\n",
      "batch len: 2048 | batch latency: 2.368058681488037s | per_sample: 0.0011562786530703306s | throughput: 864.8434331505167 samples/s\n",
      "batch len: 2048 | batch latency: 2.3680970668792725s | per_sample: 0.0011562973959371448s | throughput: 864.8294145724765 samples/s\n",
      "batch len: 2048 | batch latency: 2.367774724960327s | per_sample: 0.0011561400024220347s | throughput: 864.9471499170238 samples/s\n"
     ]
    }
   ],
   "source": [
    "runtime = time.time()\n",
    "embeddings = infer(e5_model_ipu, poptorch_dataloader)\n",
    "runtime = time.time() - runtime\n",
    "\n",
    "e5_model_ipu.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903931f0-bb08-472a-a33d-6ff29b773a9e",
   "metadata": {},
   "source": [
    "Lets print out one of the results, and the total IPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b170bf1a-1db6-4d8b-9099-479eb616bb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPU runtime: 9.597113132476807\n",
      " First embedding: tensor([-0.0108, -0.0634,  0.0363,  ...,  0.0274, -0.0333, -0.0008],\n",
      "       dtype=torch.float16)\n",
      " Shape: torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "print(f\"IPU runtime: {runtime}\\n First embedding: {embeddings[0]}\\n Shape: {embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353baa3c-39ce-4e6f-9500-d3cc3831c608",
   "metadata": {},
   "source": [
    "The embedding vector in its current state doesn't look particularly meaningful. The embeddings for a single sequence represent low-dimensional numerical representations of the word-level and sentence-level context for each token. These pre-trained embeddings can be used in applications like embedding retrieval for recommender systems, or semantic search for query-matching using cosine-similarity. Both of these use cases take advantage of the generated embeddings space, by performing a relative comparison of the user input sequence embeddings using some proximity metric.\n",
    "\n",
    "We'll use the open source `sentence_transformers` library which provides utilities for embeddings tasks to perform semantic search on a user query to retrieve the most similar sequences from the dataset to the query. This is a helpful utility for making, for example, more responsive FAQs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd0341f-1ddf-4ddd-a224-663ca6c5d290",
   "metadata": {},
   "source": [
    "## Semantic search with E5 generated embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e889bd-47b4-4fc6-805f-deffe287b717",
   "metadata": {},
   "source": [
    "Using the `go_emotions` dataset, lets create a simple similarity search engine using `sentence_transformers` semantic search function, which uses cosine similarity to retrieve close-proximity sentences from a given set of embeddings to a given query. We have already generated embeddings for the dataset, so the next step is to do the same with a given query and perform the search.\n",
    "\n",
    "First, to process the query, we need to tokenize it and convert it to a single-batch input for the model. This has been wrapped into a simple function which tokenizes and prepares a dictionary of model inputs (`input_ids`, `attention_mask`, etc.,) to which we just need to pass a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3efe05fe-8e30-4d17-ade7-e92df76fd6ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_query(query: str):\n",
    "    t_query = tokenizer(\n",
    "            query,\n",
    "            max_length=max_seq_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    return {k: torch.as_tensor([t_query[k]]) for k in t_query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001da157-186d-484e-abd8-b3373063deba",
   "metadata": {},
   "source": [
    "Next, to perform inference with a single input (i.e., effective batch size of 1) we re-instantiate the model by setting all device batching, replication and micro batch-size to 1 and re-compile the model. The change in batch size necessitates a recompilation, since the input shape to the model has been changed. We will follow the steps to initiate the model outlined earlier in the notebook, but forcing the `get_ipu_config` function to have all batching turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a81edc13-236e-4577-bbf4-01904cc09777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0124, -0.0453,  0.0185,  ..., -0.0058,  0.0110,  0.0155]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipu_config = get_ipu_config(pod_type, n_ipu=1, device_iterations=1, replication_factor=1, random_seed=random_seed)\n",
    "\n",
    "inf_model = PipelinedE5Model.from_pretrained(model_name, config=e5_config).eval().half()\n",
    "inf_model.ipu_config = ipu_config\n",
    "\n",
    "inf_model = poptorch.inferenceModel(inf_model.parallelize(), ipu_config.to_options(for_inference=True))\n",
    "\n",
    "inf_model(**prepare_query(\"Running once to compile\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fccb8f74-3bc0-41a6-afc0-0d87d3cfd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SEARCH QUERY: Strongly disliked this action movie\n",
      "\n",
      " Result (rank 1) | Score: 0.8954101204872131 | Text: i hate this movie \n",
      "\n",
      " Result (rank 2) | Score: 0.8791762590408325 | Text: it's a bad action movie because there's no rooting interest and the spectacle is grotesque and boring . \n",
      "\n",
      " Result (rank 3) | Score: 0.8550726771354675 | Text: it is a comedy that's not very funny and an action movie that is not very thrilling ( and an uneasy alliance , at that ) . \n",
      "\n",
      " Result (rank 4) | Score: 0.8523359894752502 | Text: . . . this movie has a glossy coat of action movie excess while remaining heartless at its core . \n",
      "\n",
      " Result (rank 5) | Score: 0.8508359789848328 | Text: 'this movie sucks . ' \n",
      "\n",
      " Result (rank 6) | Score: 0.8469082117080688 | Text: this movie . . . doesn't deserve the energy it takes to describe how bad it is . \n",
      "\n",
      " Result (rank 7) | Score: 0.8436370491981506 | Text: the movie slides downhill as soon as macho action conventions assert themselves . \n",
      "\n",
      " Result (rank 8) | Score: 0.8433720469474792 | Text: a dreadful live-action movie . \n",
      "\n",
      " Result (rank 9) | Score: 0.8418941497802734 | Text: this is a train wreck of an action film -- a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs . \n",
      "\n",
      " Result (rank 10) | Score: 0.8415102958679199 | Text: . . . if you , like me , think an action film disguised as a war tribute is disgusting to begin with , then you're in for a painful ride . \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import semantic_search\n",
    "\n",
    "query = \"Strongly disliked this action movie\"\n",
    "\n",
    "query_embeddings = inf_model(**prepare_query(query))\n",
    "hits = semantic_search(query_embeddings.float(), embeddings.float(), top_k=10)\n",
    "\n",
    "print(f\"\\n SEARCH QUERY: {query}\")\n",
    "for n, res in enumerate(hits[0]):\n",
    "    print(f\"\\n Result (rank {n+1}) | Score: {res['score']} | Text: {dataset['train']['text'][res['corpus_id']]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d439ae-5902-47d5-9ff3-c0bef5b22aa9",
   "metadata": {},
   "source": [
    "From the results, the pretrained embeddings appear to perform quite well on an unseen dataset without any fine-tuning. Lets turn our semantic-search example into a neat Gradio app to demonstrate a mini \"search engine\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdbef3-898c-4e11-9fa0-a02093ad45ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7864\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "def e5_semantic_search(query):\n",
    "    query_embeddings = inf_model(**prepare_query(query))\n",
    "    hits = semantic_search(query_embeddings.float(), embeddings.float(), top_k=5)\n",
    "\n",
    "    results = {'text':[], 'score':[]}\n",
    "    for n, res in enumerate(hits[0]):\n",
    "        results['text'].append(f\"{dataset['train']['text'][res['corpus_id']]}\")\n",
    "        results['score'].append(res['score'])\n",
    "        \n",
    "    return f\"{results}\"\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=e5_semantic_search,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Really liked this action movie\"),\n",
    "    outputs=gr.Textbox(lines=2, placeholder=\"what\")\n",
    ")\n",
    "\n",
    "demo.launch(server_name='0.0.0.0', share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6e6a5b-7ce8-4dc8-a33e-08c06ff203d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
